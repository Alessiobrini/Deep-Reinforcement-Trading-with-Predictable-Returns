# -*- coding: utf-8 -*-
"""
Created on Wed Dec 11 20:00:44 2019

@author: aless
"""
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.offsetbox import AnchoredText

import seaborn 
seaborn.set_style('darkgrid')
plt.rcParams['figure.figsize'] = (20.0, 10.0)
plt.rcParams['savefig.dpi'] = 90
plt.rcParams['font.family'] = 'serif'
plt.rcParams['font.size'] = 14

from statsmodels.tsa.stattools import adfuller

import sys
import pdb
import pickle
import os
from tqdm import tqdm

from utils.format_tousands import format_tousands
from utils.readYaml import saveConfigYaml, readConfigYaml


class QTraderObject(object):

    """
    A class used to implement a Reinforcement Learning agent on the basis of the
    experiments conducted in Ritter (2017), which used Qlearning algorithm.
    The class is easily extendible to different algorithm to perform the same
    experiments. The result of RL agents can be compare to the optimal analytical
    solution based on Dynamic Programming by Garleanu and Pedersen (2013).
    ...

    Attributes
    ----------
    Param : dict
        a dictionary containing parameters for the experiment. It is passed to
        the code by a yaml file
    ParamSpace : dict
        a dictionary containing the discrete space for the space and action 
        representation. It is generated by a method called _setSpaces which makes
        use of the Param dictionary


    Methods
    -------
    find_nearest_return(value)
        Deal with simulated returns that fall within the predefined ParamSpace
        
    find_nearest_holding(value)
        Deal with portfolio holdings that fall within the predefined ParamSpace
        
    ReturnSampler(sampleSize, seed=None)
        Simulate stochastic processes to get return, according to a selected dynamic.
        The returns are obtained as a sum of mean reverting stochastic processes
        each of them a specific factor with different mean reversion rate
    
    TotalCost(dn)
        Calculate transaction costs according to a selected function (for now 
        it is quadratic)
        
    GetReward(currState, nextState)
        Compute the one step reward function given the current and the next state
        representation
    
    CreateQTable()
        Create a pandas dataframe representing the Q function for all possible 
        state-action cases 
        
    getQvalue(state)
        Get the set of action available in that state 
        
    argmaxQ(state)
        Get the action that maximizes reward in the given state 
    
    getMaxQ(state)
        Get the action-value function for the action that maximizes it 
        in the given state 
        
    chooseAction(state)
        Greedy way to pick the action at every state
    
    OptTradingRate()
        Compute the optimal trading rate following the analytical solution of
        Garleanu Pedersen. 
        
    ChooseOptimalHolding(OptCurrHolding, CurrFactors, CurrRet, OptRate)    
        Compute the optimal reward using the optimal analytical solution 
        by Garleanu Pedersen
        
    QLearning(seed=None)
        Perform QLearning algorithm for the given dictionary of parameters.
        The method update the QTable function according to the Qlearning algorithm and produce
        also the results using the optimal analytical solution in order to have a benchmark.
        Depending on the Param dict choices, it is able to plot intermediate runtime results
        and a final plot comparing it with the abovementioned benchmark. Moreover, it stores
        the time series resulting from the training and the learned Qtable.
        
    TrainQTrader()
        Run the algorithm and plot different results for both the RL agent and the
        optimal analytical solution. It generates the desidered tree of folder in which it stores
        the parameter config and the results of the algorithm.
    
    PlotLearningResults(res_df, title, plot_GP, iteration)
        Generate intermediate plot of the results during runtime or in aggregate
        with respect to the optimal benchmark after the training
    
    PlotLearningSpeed(res_df,title,iteration)
        Generate plot of the rate of learning of the algorithm after training
    
    ComputeSharpeRatio(series)
        Compute SharpeRatio measure from a pnl series
    
    ComputeLearningSpeed(res_df,series_name)
        Compute learning speed measure from a pnl series
        
    GeneratePathFolder()
        Create proper directory tree for storing results
    """

    # ----------------------------------------------------------------------------
    # Init method      
    # ----------------------------------------------------------------------------
    def __init__(self, Param):

        '''
        init method to initialize the class. Parameter inputs are stored 
        as properties of the object.
        '''
        self.Param = Param
        if Param['trainable'] == 1:
            self._setSpaces(Param)
        
    # ----------------------------------------------------------------------------
    # Private method      
    # ----------------------------------------------------------------------------
    def _setSpaces(self, Param):
        
        """
        Create discrete action, holding and price spaces
        
        
        """
         
        ParamSpace = {'A_space' : np.arange(-Param['K'] ,Param['K']+1, Param['LotSize']),
                      'H_space' : np.arange(-Param['M'],Param['M']+1, Param['LotSize']),
                      'R_space' : np.arange(-Param['R'],Param['R']+1)*Param['TickSize']}
                      
        self.ParamSpace = ParamSpace

    # ----------------------------------------------------------------------------
    # Public method       
    # ----------------------------------------------------------------------------
    
    # ROUNDING FUNCTIONS
    
    def find_nearest_return(self, value):
        '''
        Deal with simulated returns that fall within the predefined ParamSpace 
        
        Parameters
        ----------
        value : float
            The value that need to be checked if it is in the valid space
        
        Returns
        -------
        float or int
            Value in the valid space
        '''
        array = np.asarray(self.ParamSpace['R_space'])
        idx = (np.abs(array - value)).argmin()
        return array[idx]


    def find_nearest_holding(self, value):
        '''
        Deal with portfolio holdings that fall within the predefined ParamSpace 
        
        Parameters
        ----------
        value : float
            The value that need to be checked if it is in the valid space
        
        Returns
        -------
        float or int
            Value in the valid space
        '''
        array = np.asarray(self.ParamSpace['H_space'])
        idx = (np.abs(array - value)).argmin()
        return array[idx]

    # PRICE SAMPLERS
    
    def ReturnSampler(self, sampleSize, seed=None):
      
        '''
        Simulate stochastic processes to get return, according to a selected dynamic.
        The returns are obtained as a sum of mean reverting stochastic processes
        each of them a specific factor with different mean reversion rate
        
        The method can perform also an ADF test for stationarity at the end of the simulation
        but at the time of writing the code is commented (TODO eventually add a boolean)
        

        Parameters
        ----------
        sampleSize : int
            The desired lenght for the generated return series.
        seed : float or int
            The seed to reproduce the same random simulation. Default is None because
            we do not want to fix the seed in every case.
        
        The attribute Param dict passes some important parameters to the method
        
        sigmaf : float
            volatility of factor's variation
        Numfactors : int
            Number of factors to simulate (1 to 3). More than 3 causes exiting
            the script and print a Warning message
        HalfLife : int
            Half life of mean reversion for each factor involved in the simulation.
            Used to compute the speed of mean reversion
        f_param : float
            Factor coefficients for each factor involved in the simulation.
            Used to compute returns.
        add_noise : bool
            boolean to choose if add noise to the return simulation
        sigma : float
            volatility for the noise in return simulation
        f_init : float
            initialization points for factor simulation (generally 0)
        plot_inputs : bool
            boolean to decide if plotting simulated returns for developing purposes.
            At the moment if you decide to plot, then you exit the runtime. TODO change it in the future
        
        Returns
        -------
        realret
            array of simulated returns used by the optimal analytical solution
        capret
            array of simulated returns capped to the valid Paramspace used by
            the RL agent
        factors
            array of simulated factors that explain returns
        '''
        # Set seed to make the out-of-sample experiment reproducible
        np.random.seed(seed)
        
        # use samplesize +2 because when iterating the algorithm is necessary to 
        # have one observation more (the last space representation) and because
        # we want be able to plot insample operation every tousand observation.
        # Therefore we don't want the index ending at 999 instead of 1000
        
        # Generate stochastic factor component and compute speed of mean reversion
        eps = np.random.randn(sampleSize + 2) 
        sigmaf = self.Param['sigmaf']
        lambdas = np.around(np.log(2)/self.Param['HalfLife'],4)
        
        # if condition to enable simulations with different factors
        if self.Param['NumFactors'] == 1:
            # simulate the single factor according to OU process
            # select proper speed of mean reversion and initialization point
            # it is faster to increase the size of a python list than a numpy array
            # therefore we convert later the list
            lambda_5D = lambdas
            f0_5D = self.Param['f_init']
            f5D = []
            for i in tqdm(iterable=range(sampleSize + 2), desc='Simulating Factors'):
                f1_5D = (1 - lambda_5D) * f0_5D + sigmaf * eps[i]
                f5D.append(f1_5D)
                f0_5D = f1_5D

            # convert factor list to numpy array
            factors  = np.array(f5D) 

            # instantiate factor parameter for regression
            f_param = np.array(self.Param['f_param'])

            # calculate return series depending on boolean for adding noise
            if self.Param['add_noise']:
                
                u = np.random.randn(sampleSize + 2)
                sigma = self.Param['sigma']
                realret = [(f_param * factors[i]) 
                           + sigma * u[i] for i in range(sampleSize + 2)]
            else:
                realret = [(f_param * factors[i]) for i in range(sampleSize + 2)]

            capret = [self.find_nearest_return(r) for r in realret]
                        
            self.f_speed = np.array([lambda_5D])

            

        elif self.Param['NumFactors'] == 2:
            
            lambda_5D = lambdas[0]
            lambda_1Y = lambdas[1]
     
            f0_5D = self.Param['f_init'][0]
            f5D = []
            f0_1Y = self.Param['f_init'][1] 
            f1Y = []

            for i in tqdm(iterable = range(sampleSize + 2), desc='Simulating Factors'):
                # first factor
                f1_5D = (1 - lambda_5D) * f0_5D + sigmaf * eps[i]
                f5D.append(f1_5D)
                f0_5D = f1_5D
                # second factor
                f1_1Y = (1 - lambda_1Y) * f0_1Y + sigmaf * eps[i]
                f1Y.append(f1_1Y)
                f0_1Y = f1_1Y        
            
            factors = np.concatenate([np.array(f5D).reshape(-1,1),
                                      np.array(f1Y).reshape(-1,1)],axis=1)
            
            f_param = np.array(self.Param['f_param'])[:2]
            
            
            if self.Param['add_noise']:
                
                u = np.random.randn(sampleSize + 2)
                sigma = self.Param['sigma']
                realret = [(f_param @ factors[i]) 
                           + sigma * u[i] for i in range(sampleSize + 2)]
            else:
                realret = [(f_param @ factors[i]) for i in range(sampleSize + 2)]            
            
            capret = [self.find_nearest_return(r) for r in realret]
            
            
            self.f_speed = np.array([lambda_5D,lambda_1Y])
            
        elif self.Param['NumFactors'] == 3:

         
            # simulate factors according to OU process
            lambda_5D = lambdas[0]
            lambda_1Y = lambdas[1]
            lambda_5Y = lambdas[2]
            
            f0_5D = self.Param['f_init'][0]
            f5D = []
            f0_1Y = self.Param['f_init'][1] 
            f1Y = []
            f0_5Y = self.Param['f_init'][2] 
            f5Y = []
            
            for i in tqdm(iterable = range(sampleSize + 2), desc='Simulating Factors'):
                # first factor
                f1_5D = (1 - lambda_5D) * f0_5D + sigmaf * eps[i]
                f5D.append(f1_5D)
                f0_5D = f1_5D
                # second factor
                f1_1Y = (1 - lambda_1Y) * f0_1Y + sigmaf * eps[i]
                f1Y.append(f1_1Y)
                f0_1Y = f1_1Y
                # third factor
                f1_5Y = (1 - lambda_5Y) * f0_5Y + sigmaf * eps[i]
                f5Y.append(f1_5Y)
                f0_5Y = f1_5Y
            
            factors = np.concatenate([np.array(f5D).reshape(-1,1),
                                      np.array(f1Y).reshape(-1,1),
                                      np.array(f5Y).reshape(-1,1)],axis=1)
            
            f_param = np.array(self.Param['f_param'])
            
            if self.Param['add_noise']:
                
                u = np.random.randn(sampleSize + 2)
                sigma = self.Param['sigma']
                realret = [(f_param @ factors[i]) 
                           + sigma * u[i] for i in range(sampleSize + 2)]
            else:
                realret = [(f_param @ factors[i]) for i in range(sampleSize + 2)]  
            
            capret = [self.find_nearest_return(r) for r in realret]
                
            
            self.f_speed = np.array([lambda_5D,lambda_1Y,lambda_5Y])
    
        
        else:
            
            print('##############################################################################')
            print('NumFactors greater than 3! Check the dimensionality or implement more factors!')
            print('##############################################################################')
            sys.exit()
            
        print(str(len(self.f_speed)) + ' factor(s) simulated')
        print('max realret ' + str(max(realret)))
        print('min realret ' + str(min(realret)))
        print('################################################################')
        print('max capret ' + str(max(capret)))
        print('min capret ' + str(min(capret)))
        print('################################################################')
        print(self.ParamSpace['R_space'])
        
        pdb.set_trace()
        
        # plots for factor, returns and prices
        if self.Param['plot_inputs']:
            fig1 = plt.figure()
            fig2 = plt.figure()

            ax1 = fig1.add_subplot(111)
            ax2 = fig2.add_subplot(111)
                
            ax1.plot(factors)
            ax1.legend(['5D','1Y','5Y'])
            ax1.set_title('Factors')
            ax2.plot(capret)
            ax2.plot(realret,alpha=0.7)
            plt.legend(['CapReturns','Returns'])
            ax2.set_title('Returns')
            

            # test=adfuller(realret)
            # print('Test ADF for generated return series')
            # print("Test Statistic: " + str(test[0]))
            # print("P-value: " + str(test[1]))
            # print("Used lag: " + str(test[2]))
            # print("Number of observations: " + str(test[3]))
            # print("Critical Values: " + str(test[4]))
            # print("AIC: " + str(test[5]))
            sys.exit()
        
        
        
        return realret,capret,factors


    # COST FUNCTIONS
    
    def TotalCost(self,dn):
        '''
        Calculate transaction costs according to a selected function (for now 
        it is quadratic)
        
        Parameters
        ----------
        dn : int
            Traded quantity between two periods
        
        Returns
        -------
        float
            Value of the quadratic cost function for a given quantity
        '''
        
        # check which noise to add for computing the optimal solution
        if self.Param['add_noise']:
            sigma = self.Param['sigma']
        else:
            sigma = self.Param['sigmaf']
        
        # calculate Kyle's Lambda (Garleanu Pedersen 2013)
        Lambda = self.Param['CostMultiplier'] * sigma**2
        quadratic_costs = 0.5 * (dn**2) * Lambda
        
        return quadratic_costs

    
    # REWARD FUNCTIONS
    
    def GetReward(self, currState, nextState):

        '''
        Compute the one step reward function given the current and the next state
        representation
        
        Parameters
        ----------
        currState : tuple of float or int
            Tuple representing the current state of the environment for the RL agent
        
        nextState : tuple of float or int
            Tuple representing the one steap ahead state of the environment 
            for the RL agent
            
        Returns
        -------
        Result
            dict of values containing relevant quantities to store during the 
            runtime of the RL algorithm
        '''
        
        # Remember that a state is a tuple (price, holding)
        currRet = currState[0]
        nextRet = nextState[0]
        currHolding = currState[1]
        nextHolding = nextState[1]
        
        #pdb.set_trace()
        # Traded quantity between period
        dn = nextHolding - currHolding
        # Portfolio variation
        GrossPNL = nextHolding * nextRet
        # Risk
        Risk = 0.5 * self.Param['kappa'] * ((nextHolding**2) * (self.Param['sigma']**2))
        # Transaction costs
        Cost = self.TotalCost(dn)
        # Portfolio Variation including costs
        NetPNL = GrossPNL - Cost
        # Compute reward    
        Reward = GrossPNL - Risk - Cost
        
        #pdb.set_trace()
        
        # Store quantities
        Result = {
                  'CurrHolding': currHolding,
                  'NextHolding': nextHolding,
                  'Action': dn,
                  'GrossPNL': GrossPNL,
                  'NetPNL': NetPNL,
                  'Risk': Risk,
                  'Cost': Cost,
                  'Reward' : Reward,
                  }
        return Result


    # Q TABLE GENERATOR
    
    def CreateQTable(self):
        '''
        Create a pandas dataframe representing the Q function for all possible 
        state-action cases.
        
        Parameters
        ----------
        ParamSpace : dict
            Attribute of the class containing the the discrete space for the 
            space and action representation
        
        Returns
        -------
        Pandas Dataframe
            Store QTable as attribute of the class

        '''
        # generate row index of the dataframe with every possible combination
        # of state space variables
        iterables = [self.ParamSpace['R_space'], self.ParamSpace['H_space']]
        State_space = pd.MultiIndex.from_product(iterables)
        
        # Create dataframe and set properly index and column heading
        Q_space = pd.DataFrame(index = State_space, columns = self.ParamSpace['A_space']).fillna(0)
        Q_space.index.set_names(['Return','Holding'],inplace=True)
        Q_space.columns.set_names(['Action'],inplace=True)
        # initialize the Qvalues for action 0 as slightly greater than 0 so that
        # 'doing nothing' becomes the default action, instead the default action to be the first column of
        # the dataframe.
        Q_space[0] = 0.0000000001

        self.Q_space = Q_space

    # Q TABLE FUNCTIONS

    def getQvalue(self,state):

        ''' 
        Get the set of action available in that state 
        
        Parameters
        ----------
        state : tuple of float or int
            Tuple representing a state of the environment for the RL agent
        
        Returns
        -------
        slice of a Dataframe
            Possible set of action for a given state
        '''

        ret = state[0]
        holding = state[1]
        return self.Q_space.loc[(ret, holding),]

    def argmaxQ(self,state):

        ''' 
        Get the action that maximizes reward in the given state 
        
        Parameters
        ----------
        state : tuple of float or int
            Tuple representing a state of the environment for the RL agent
        
        Returns
        -------
        int
            Action that maximized the Q function for the given state
        '''

        return self.getQvalue(state).idxmax()

    def getMaxQ(self,state):

        ''' 
        Get the action-value function for the action that maximizes it 
        in the given state 
        
        Parameters
        ----------
        state : tuple of float or int
            Tuple representing a state of the environment for the RL agent
        
        Returns
        -------
        float
            Value function computed for for the action that maximizes it 
        in the given state 
        '''

        return self.getQvalue(state).max()


    def chooseAction(self,state):

        ''' 
        Greedy way to pick the action at every state 
        
        Parameters
        ----------
        state : tuple of float or int
            Tuple representing a state of the environment for the RL agent
        epsilon : float
            Float number representing the probability to pick one action at
            random instead of the maximizing one
        
        Returns
        -------
        int
            Quantity to trade between periods
        '''

        random_action = np.random.rand()
        if (random_action < self.Param['epsilon']):
            # pick one action at random for exploration purposes
            A_space = self.ParamSpace['A_space'] 
            #pdb.set_trace()
            dn = A_space[np.random.randint(len(A_space))]
            #pdb.set_trace()
        else:
            # pick the greedy action
            dn = self.argmaxQ(state)

        return dn


    # FUNCTION FOR COMPUTING OPTIMAL SOLUTION OF GARLEANU-PEDERSEN
    
    def OptTradingRate(self):
    
        '''
        Compute the optimal trading rate following the analytical solution of
        Garleanu Pedersen. 
        
        Parameters (given by the Param dict attribute)
        ----------
        DiscountRate : float
            Discount factor for time value of money
            
        kappa : float
            Risk aversion of the agent
            
        CostMultiplier : float
            Sensitivity wrt the trading costs. In Garleanu-Pedersen the 
            cost matrix is defined as proportional to the covariance matrix
            by a CostMultiplier
        
        Returns
        -------
        float
            Value for the optimal trading rate
        
        '''
        
        # 1 percent annualized discount rate (same rate of Ritter)
        rho = 1 - np.exp(-self.Param['DiscountRate']/260)  
        
        # kappa is the risk aversion, CostMultiplier the parameter for trading cost
        num1 = (self.Param['kappa']* ( 1 - rho) + self.Param['CostMultiplier']*rho)
        num2 = np.sqrt(num1**2 + 4 *self.Param['kappa'] * self.Param['CostMultiplier']* (1 - rho)**2)
        den = 2* (1 - rho)
        a = (-num1 + num2)/ den
        
        OptRate = a /self.Param['CostMultiplier']
    
        return OptRate
    
    def ChooseOptimalHolding(self, OptCurrHolding, CurrFactors, CurrRet, OptRate):

        '''
        Compute the optimal reward using the optimal analytical solution 
        by Garleanu Pedersen
        
        Parameters
        ----------
        OptCurrHolding : float
            Amount currently hold as a portfolio
            
        CurrFactors : float
            Current value for the explanatory variable of returns (factors)
            
        CurrRet : float
            Current value of return for the asset
            
        OptRate : float
            Optimal trading rate previously computed by the corresponding method
            
        NumFactors : int
            Number of factors involved in the simulation. The code is different because
            the one factor case does not involve matrix multiplication
        
        Returns
        -------
         Result
            dict of values containing relevant quantities to store during the 
            runtime of the RL algorithm
        '''
                
        if self.Param['NumFactors'] == 1:
            
            # Computed discounted factor loadings according to their speed of mean reversion and 
            # the optimal trading rate
            # we could also take out this computation fromt the function because the discfactor depends on fixed params
            DiscFactorLoads = self.Param['f_param']/ (1 + self.f_speed * ((OptRate * self.Param['CostMultiplier']) / \
                                                                             self.Param['kappa']))
            # check which noise to add for computing the optimal solution
            if self.Param['add_noise']:
                sigma = self.Param['sigma']
            else:
                sigma = self.Param['sigmaf']
                
                
            # Optimal traded quantity between period
            OptNextHolding = (1 - OptRate) * OptCurrHolding + OptRate * \
                          (1/(self.Param['kappa'] * (sigma)**2)) * \
                           (DiscFactorLoads * CurrFactors)
                           
            # Traded quantity as for the Markovitz framework  (Mean-Variance framework)            
            MVNextHolding =  (1/(self.Param['kappa'] * (sigma)**2)) * \
                           (self.Param['f_param'] * CurrFactors)
                       
        else:
            
            # check which noise to add for computing the optimal solution
            # TODO check how to add multiple noises and correct for the case of numfactors=2
            if self.Param['add_noise']:
                sigma = self.Param['sigma']
            else:
                sigma = self.Param['sigmaf']
            
            DiscFactorLoads = self.Param['f_param']/ (1 + self.f_speed * OptRate)
                

            OptNextHolding = (1 - OptRate) * OptCurrHolding + OptRate * \
                          (1/(self.Param['kappa'] * (self.Param['sigma'])**2)) * \
                           (DiscFactorLoads @ CurrFactors)
                      
            MVNextHolding =  (1/(self.Param['kappa'] * (self.Param['sigma'])**2)) * \
                           (self.Param['f_param'] @ CurrFactors)

        # Traded quantity between period
        OptNextAction = OptNextHolding - OptCurrHolding
        # Portfolio variation
        OptGrossPNL = OptNextHolding * CurrRet #(DiscFactorLoads * CurrFactors)
        # Risk
        OptRisk = 0.5 * self.Param['kappa'] * ((OptNextHolding)**2 * (self.Param['sigma'])**2)
        # Transaction costs
        OptCost = self.TotalCost(OptNextHolding - OptCurrHolding)
        # Portfolio Variation including costs
        OptNetPNL = OptGrossPNL - OptCost
        # Compute reward    
        OptReward = OptGrossPNL - OptRisk - OptCost
        
        # Store quantities
        Result = {
                  'OptNextAction': OptNextAction,
                  'OptCurrHolding': OptCurrHolding,
                  'OptNextHolding': OptNextHolding,
                  'OptGrossPNL': OptGrossPNL,
                  'OptNetPNL': OptNetPNL,
                  'OptRisk': OptRisk,
                  'OptCost': OptCost,
                  'OptReward' : OptReward,
                  'MVNextHolding' : MVNextHolding
                  }
        return Result
    
    # TRAIN FUNCTION
    
    def QLearning(self, seed=None):

        '''
        Perform QLearning algorithm for the given dictionary of parameters.
        The method update the QTable function according to the Qlearning algorithm and produce
        also the results using the optimal analytical solution in order to have a benchmark.
        Depending on the Param dict choices, it is able to plot intermediate runtime results
        and a final plot comparing it with the abovementioned benchmark. Moreover, it stores
        the time series resulting from the training and the learned Qtable.
        
        Parameters
        ----------
        seed : float or int
            The seed to reproduce the same random simulation. Default is None because
            we do not want to fix the seed in every case.
        
        Returns
        -------
        float
            Value function computed for for the action that maximizes it 
        in the given state 
        '''
        
        # generate a sample
        realret, ret, factors = self.ReturnSampler(self.Param['N_train'] , seed)
        # initialize dataframe
        res_df = pd.DataFrame(np.concatenate([np.array(ret).reshape(-1,1),
                                              np.array(factors).reshape(-1,1)], 
                                             axis=1), 
                              columns = ['returns','factors'])
        
        # initialize holding for the RL agent and the Optimal Agent
        currHolding = self.find_nearest_holding(self.Param['Startholding'])
        curroptHolding = self.find_nearest_holding(self.Param['Startholding'])
        
        # Compute optimal trading rate for analytical solution
        OptRate = self.OptTradingRate()
        
        for i in tqdm(iterable=range(self.Param['N_train']+1), desc='Training QLearning'):
            ##########################################################################à
            # RL solution
            # represent the current state as tuple
            if self.Param['executeRL'] == 1:
                currRet = ret[i]
                currState = (currRet, currHolding)
    
                # choose action
                shares_traded = self.chooseAction(currState)
                # represent the next state as tuple
                nextHolding = self.find_nearest_holding(currHolding + shares_traded)
                nextRet = ret[i+1]
                nextState = (nextRet, nextHolding)
                #pdb.set_trace()
                # compute reward and update the Q function
                Result = self.GetReward(currState, nextState)
                
                if self.Param['trainable'] == 1:
                    q_sa = self.Q_space.loc[currState, shares_traded]
                    increment = self.Param['alpha'] * ( Result['Reward'] + \
                                          self.Param['gamma'] * self.getMaxQ(nextState) - q_sa)
                    self.Q_space.loc[currState, shares_traded] = q_sa + increment
                
                # store quantities from the dictionary Result in the Dataframe
                if i==0:
                    for key in Result.keys(): 
                        res_df[key] = 0
                        #pdb.set_trace()
                        res_df.at[i,key] = Result[key]
                else:
                    for key in Result.keys(): 
                        res_df.at[i,key] = Result[key]
                
                # go to the nextholding for the subsequent step of the for loop
                currHolding = nextHolding
                
                # produce in_sample plot
                if self.Param['plot_insample']:
                    if (i % (self.Param['N_train']/5) == 0) & (i != 0):
                        self.PlotLearningResults(res_df.loc[:i], title='Qlearning iteration ' + str(i),
                                                 iteration=i)
            
            ##########################################################################à
            # Optimal analytical solution
            if self.Param['execute_GP'] == 1:

                currFactors = factors[i]
                currReturn = realret[i]
        
                OptResult = self.ChooseOptimalHolding(curroptHolding, currFactors, currReturn, OptRate)
                    
                if i==0:
                    for key in OptResult.keys(): 
                        res_df[key] = 0                 
                        res_df.at[i,key] = OptResult[key]
                else:
                    for key in OptResult.keys():
                        res_df.at[i,key] = OptResult[key]
        
                curroptHolding = OptResult['OptNextHolding']
                
                # plot results including analytical solution     
        
        if self.Param['execute_GP']:
            self.PlotLearningResults(res_df, title='Q learning with benchmark', 
                                     plot_GP=self.Param['execute_GP'], 
                                     iteration = self.Param['N_train'])

        # store QTable
        if self.Param['save_table'] == 1:
            # convert columns name to string header because parquet doesn't support int
            self.Q_space.columns = self.Q_space.columns.astype(str)
            
            # save results as parquet
            self.Q_space.to_parquet(os.path.join(self.savedpath,
                                             'QTable_' + format_tousands(self.Param['N_train'])
                                             + '.parquet.gzip'), compression='gzip')
        
        # store results
        if self.Param['save_results'] == 1:
            if self.Param['trainable'] == 1:
                res_df.to_parquet(os.path.join(self.savedpath,
                                  'Results_' + format_tousands(self.Param['N_train']) 
                                  + '.parquet.gzip'),compression='gzip')
            else:
                res_df.to_parquet(os.path.join(self.savedpath,
                                  'Results_Test' + format_tousands(self.Param['N_train']) 
                                  + '.parquet.gzip'),compression='gzip')   
        
    
    # LAUNCH EXPERIMENT FUNCTION
    def TrainQTrader(self):
    
        '''
        Run the algorithm and plot different results for both the RL agent and the
        optimal analytical solution. It generates the desidered tree of folder in which it stores
        the parameter config and the results of the algorithm.
        '''
        
        # Create directory for outputs and store a path
        self.GeneratePathFolder()
        
        #pdb.set_trace()
        # store parameters configuration
        saveConfigYaml(self.Param,self.savedpath)

        # Initialize QTable
        self.CreateQTable()

        # Train and produce results
        self.QLearning()
        
    def TestFixedQ(self):
        
        '''
        Run the algorithm with a pretrained QTable. The method looks into the
        desired folder, open the old parameter config and merge it with the
        initialized dictionary. Then loaded the trained QTable and use it to run 
        the same experiment with the same lenght. This method is useful when one
        want to understand at what level the learning stabilizes, comparing its results
        to those obtained during training
        '''
        
        # Create directory for outputs and store a path
        self.GeneratePathFolder()
        
        # Extract Parameters
        ParamTrain = readConfigYaml(os.path.join(self.savedpath,
                                                 '_'.join(['config',
                                                           format_tousands(self.Param['N_train'])]) 
                                                 + '.yaml'))
        
        # Merge dictionaries
        NewParam = {**self.Param, **ParamTrain}
        NewParam['plot_insample'] = 0
        NewParam['trainable'] = 0
        NewParam['save_table'] = 0
        NewParam['execute_GP'] = 1
        self.Param = NewParam
        
        # Load QTable from file and instantiate it as attribute
        self.Q_space = pd.read_parquet(os.path.join(self.savedpath,
                                              '_'.join(['QTable',
                                                        format_tousands(self.Param['N_train'])]) + 
                                              '.parquet.gzip'))
        
        # convert back QTable columns as int and extract ParamSpaces 
        self.Q_space.columns = self.Q_space.columns.astype(int)
        self.getSpaces()
        
        # Train and produce results
        self.QLearning()
        
        
        
    # MONITORING FUNCTIONS
    def PlotLearningResults(self, res_df, title, iteration, plot_GP=0):
        
        '''
        Generate intermediate plot of the results during runtime or in aggregate
        with respect to the optimal benchmark after the training.
        
        Parameters
        ----------
        res_df: pandas dataframe
            Dataframe containing time series of results to plot
        
        title: str
            Title selected for the plot
        
        iteration: int
            Number of iteration to distinguish different steps in the learning process
        
        plot_GP=0 : bool
            Boolean to decide if plot thelearning results wrt the GP benchmark.
            Default is not plotting it.
        
        Returns
        -------
        Produce and save plot
        '''

   
        if plot_GP:
            
            ############################################################################
            # first figure GP
            fig = plt.figure(figsize=(34,13))
            fig.tight_layout()
            plt.suptitle(title,fontsize=28)
    
            # first plot
            axpnl = fig.add_subplot(2,2,1)
            axpnl.plot(res_df['OptGrossPNL'].cumsum(), label = 'OptGrossPNL', color = 'orange')
            axpnl.set_title('GrossPNL')
            
            #second plot
            axnetpnl = fig.add_subplot(2,2,2)
            axnetpnl.plot(res_df['OptNetPNL'].cumsum(), label = 'OptNetPNL', color = 'orange')
            axnetpnl.set_title('NetPNL')
            
            #third plot
            axreward = fig.add_subplot(2,2,3)
            axreward.plot(res_df['OptReward'].cumsum(), label = 'OptReward', color = 'orange')
            axreward.set_title('Reward')
            
            
            #fourth plot
            axcumcost = fig.add_subplot(2,2,4)
            axcumcost.plot(res_df['OptCost'].cumsum(), label = 'OptCost', color = 'orange')
            axcumcost.set_title('Cumulative Cost')
            
            
            if self.Param['executeRL']:
                axpnl.plot(res_df['GrossPNL'].cumsum(), label = 'GrossPNL', color = 'blue')
                
                GrossPNLmean, GrossPNLstd, GrossPNLsr = self.ComputeSharpeRatio(res_df['GrossPNL'])
                OptGrossPNLmean, OptGrossPNLstd, OptGrossPNLsr = self.ComputeSharpeRatio(res_df['OptGrossPNL'])
                grosspnl_text = AnchoredText(' Gross Sharpe Ratio: ' + str(np.around(GrossPNLsr,2)) + 
                                              '\n Gross PnL mean: ' + str(np.around(GrossPNLmean,2)) + 
                                              '\n Gross PnL std: ' + str(np.around(GrossPNLstd,2)) +
                                              '\n OptGross Sharpe Ratio: ' + str(np.around(OptGrossPNLsr,2)) + 
                                              '\n OptGross PnL mean: ' + str(np.around(OptGrossPNLmean,2)) + 
                                              '\n OptGross PnL std: ' + str(np.around(OptGrossPNLstd,2)), 
                                              loc=4, prop=dict(size=10)) # pad=0, borderpad=0, frameon=False 
    
                axpnl.add_artist(grosspnl_text)
                
                
                axnetpnl.plot(res_df['NetPNL'].cumsum(), label = 'NetPNL', color = 'blue')
                
                NetPNLmean, NetPNLstd, NetPNLsr = self.ComputeSharpeRatio(res_df['NetPNL'])
                OptNetPNLmean, OptNetPNLstd, OptNetPNLsr = self.ComputeSharpeRatio(res_df['OptNetPNL'])
                netpnl_text = AnchoredText(' Net Sharpe Ratio: ' + str(np.around(NetPNLsr,2)) + 
                                              '\n Net PnL mean: ' + str(np.around(NetPNLmean,2)) + 
                                              '\n Net PnL std: ' + str(np.around(NetPNLstd,2)) +
                                              '\n OptNet Sharpe Ratio: ' + str(np.around(OptNetPNLsr,2)) + 
                                              '\n OptNet PnL mean: ' + str(np.around(OptNetPNLmean,2)) + 
                                              '\n OptNet PnL std: ' + str(np.around(OptNetPNLstd,2)), 
                                              loc=4,  prop=dict(size=10) )
                axnetpnl.add_artist(netpnl_text)
                
                axreward.plot(res_df['Reward'].cumsum(), label = 'Reward', color = 'blue')
                axcumcost.plot(res_df['Cost'].cumsum(), label = 'Cost', color = 'blue')
            else:
                
                OptGrossPNLmean, OptGrossPNLstd, OptGrossPNLsr = self.ComputeSharpeRatio(res_df['OptGrossPNL'])
                grosspnl_text = AnchoredText(' OptGross Sharpe Ratio: ' + str(np.around(OptGrossPNLsr,2)) + 
                                              '\n OptGross PnL mean: ' + str(np.around(OptGrossPNLmean,2)) + 
                                              '\n OptGross PnL std: ' + str(np.around(OptGrossPNLstd,2)), 
                                              loc=4, prop=dict(size=10)) # pad=0, borderpad=0, frameon=False 
    
                axpnl.add_artist(grosspnl_text)
                
                OptNetPNLmean, OptNetPNLstd, OptNetPNLsr = self.ComputeSharpeRatio(res_df['OptNetPNL'])
                netpnl_text = AnchoredText(' OptNet Sharpe Ratio: ' + str(np.around(OptNetPNLsr,2)) + 
                                              '\n OptNet PnL mean: ' + str(np.around(OptNetPNLmean,2)) + 
                                              '\n OptNet PnL std: ' + str(np.around(OptNetPNLstd,2)), 
                                              loc=4,  prop=dict(size=10) )
                axnetpnl.add_artist(netpnl_text)
             
    
            axpnl.legend() 
            axnetpnl.legend()
            axreward.legend()
            axcumcost.legend()
            
            figpath = os.path.join(self.savedpath,'GP_Qlearning_cumplot_'+ '_' +
                                    format_tousands(self.Param['N_train']) +'.PNG')
            
            # save figure
            plt.savefig(figpath)
            
            ###############################################################################
            #second figure GP
            fig2 = plt.figure(figsize=(34,13))
            fig2.tight_layout()
            plt.suptitle(title,fontsize=28)
                
            # first plot
            axcost = fig2.add_subplot(4,1,1)
            axcost.plot(res_df['OptCost'], label = 'OptCost', alpha=0.7, color = 'orange') 
            axcost.set_title('Cost', x=-0.1,y=0.5)     
            # second plot
            axrisk = fig2.add_subplot(4,1,2)
            axrisk.plot(res_df['OptRisk'], label = 'OptRisk', alpha=0.7, color = 'orange')
            axrisk.set_title('Risk', x=-0.1,y=0.5)
            # third plot
            axaction = fig2.add_subplot(4,1,3)
            axaction.plot(res_df['OptNextAction'], label = 'OptNextAction', alpha=0.7, color = 'orange')
            axaction.set_title('Action', x=-0.1,y=0.5)
            # fourth plot
            axholding = fig2.add_subplot(4,1,4)
            axholding.plot(res_df['OptNextHolding'],label = 'OptNextHolding', alpha=0.7, color = 'orange')
            axholding.set_title('Holding', x=-0.1,y=0.5)
            
            if self.Param['executeRL']:
                axcost.plot(res_df['Cost'], label = 'Cost', color = 'blue')
                axrisk.plot(res_df['Risk'], label = 'Risk', color = 'blue')
                axaction.plot(res_df['Action'], label = 'Action', color = 'blue')
                axholding.plot(res_df['NextHolding'],label = 'NextHolding', color = 'blue')


            axcost.legend()
            axrisk.legend()
            axaction.legend()
            axholding.legend()
            
            figpath = os.path.join(self.savedpath,'GP_Qlearning_plot'+ '_' +
                                    format_tousands(self.Param['N_train']) +'.PNG')
            
            # save figure
            plt.savefig(figpath)
                 
        else:
            
            ############################################################################
            # first figure Ritter
            fig = plt.figure(figsize=(34,13))
            fig.tight_layout()
            plt.suptitle(title,fontsize=28)
    
            
            # first plot
            axpnl = fig.add_subplot(2,2,1)
            axpnl.plot(res_df['GrossPNL'].cumsum(), color = 'blue')
            axpnl.set_title('GrossPNL')
            axpnl.legend(['GrossPNL'])
            GrossPNLmean, GrossPNLstd, GrossPNLsr = self.ComputeSharpeRatio(res_df['GrossPNL'])
            grosspnl_text = AnchoredText(' Gross Sharpe Ratio: ' + str(np.around(GrossPNLsr,2)) + 
                                          '\n Gross PnL mean: ' + str(np.around(GrossPNLmean,2)) + 
                                          '\n Gross PnL std: ' + str(np.around(GrossPNLstd,2)), 
                                          loc=4, prop=dict(size=10)  )
            axpnl.add_artist(grosspnl_text)
            
            # second plot
            axnetpnl = fig.add_subplot(2,2,2)
            axnetpnl.plot(res_df['NetPNL'].cumsum(), color = 'blue')
            axnetpnl.set_title('NetPNL')
            axnetpnl.legend(['NetPNL'])
            NetPNLmean, NetPNLstd, NetPNLsr = self.ComputeSharpeRatio(res_df['NetPNL'])
            netpnl_text = AnchoredText(' Net Sharpe Ratio: ' + str(np.around(NetPNLsr,2)) + 
                                          '\n Net PnL mean: ' + str(np.around(NetPNLmean,2)) + 
                                          '\n Net PnL std: ' + str(np.around(NetPNLstd,2)), 
                                          loc=4, prop=dict(size=10) )
            axnetpnl.add_artist(netpnl_text)


            # third plot 
            axreward = fig.add_subplot(2,2,3)
            axreward.plot(res_df['Reward'].cumsum(), color = 'blue')
            axreward.set_title('Reward')
            axreward.legend(['Reward'])
            
            # fourth plot
            axcumcost = fig.add_subplot(2,2,4)
            axcumcost.plot(res_df['Cost'].cumsum(), color = 'blue')
            axcumcost.set_title('CumCost')
            axcumcost.legend(['CumCost'])
            
            figpath = os.path.join(self.savedpath,'Qlearning_cumplot_'+ 
                                   '_iteration_' + str(iteration) + '_' + 
                                    format_tousands(self.Param['N_train']) +'.PNG')
            # save figure
            plt.savefig(figpath)
            
            ############################################################################
            # second figure Ritter
            fig2 = plt.figure(figsize=(34,13))
            fig2.tight_layout()
            plt.suptitle(title,fontsize=28)
            
            # first figure
            axcost = fig2.add_subplot(4,1,1)
            axcost.plot(res_df['Cost'], color = 'blue')
            axcost.set_title('Cost', x=-0.1,y=0.5)
            axcost.legend(['Cost'])
                                 
            # second plot
            axrisk = fig2.add_subplot(4,1,2)
            axrisk.plot(res_df['Risk'], color = 'blue')
            axrisk.set_title('Risk', x=-0.1,y=0.5)
            axrisk.legend(['Risk'])

            # third plot
            axaction = fig2.add_subplot(4,1,3)
            axaction.plot(res_df['Action'], color = 'blue')
            axaction.set_title('Action', x=-0.1,y=0.5)
            axaction.legend(['Action'])
            
            # fourth plot
            axholding = fig2.add_subplot(4,1,4)
            axholding.plot(res_df['NextHolding'], color = 'blue')
            axholding.set_title('Holding', x=-0.1,y=0.5)
            axholding.legend(['NextHolding'])
                        
            figpath = os.path.join(self.savedpath,'Qlearning_plot_'+ 
                                   '_iteration_' + str(iteration) + '_' + 
                                    format_tousands(self.Param['N_train']) +'.PNG')

            # save figure
            plt.savefig(figpath)
            
            

         
    # GENERAL TOOLS
    def ComputeSharpeRatio(self,series):
        
        '''
        Compute SharpeRatio measure from a pnl series
        
        Parameters
        ----------
        series : pandas series or dataframe
            series of expected returns (pnl)
        
        Returns
        -------
        mean, standard deviation and Sharpe Ratio of the series
        '''
        
        mean = np.array(series).mean()
        std = np.array(series).std()
        sr = (mean/std) * (252 ** 0.5)
        
        return mean, std, sr

    
    def GeneratePathFolder(self):
        
        '''
        Create proper directory tree for storing results
        '''
        
        # Create directory for outputs
        #pdb.set_trace()
        if self.Param['varying_par']:
            savedpath = os.path.join(os.getcwd(),
                                     self.Param['outputDir'],
                                     self.Param['outputClass'],
                                     '_'.join([self.Param['outputModel'],self.Param['varying_par']]),
                                     format_tousands(self.Param['N_train']),
                                     '_'.join([self.Param['varying_par'],
                                               str(self.Param[self.Param['varying_par']])]))
            
        else:
            savedpath = os.path.join(os.getcwd(),
                                     self.Param['outputDir'],
                                     self.Param['outputClass'],
                                     self.Param['outputModel'],
                                     format_tousands(self.Param['N_train']))
        
        # use makedirs to create a tree of subdirectory
        if not os.path.exists(savedpath):
            os.makedirs(savedpath)
            
        self.savedpath = savedpath
        
        
    def getSpaces(self):
        
        """
        Create discrete action, holding and price spaces from preexisting QTable
        """
         
        ParamSpace = {'A_space' : self.Q_space.columns,
                      'H_space' : self.Q_space.index.get_level_values(1).unique(),
                      'R_space' : self.Q_space.index.get_level_values(0).unique()}
                      
        self.ParamSpace = ParamSpace