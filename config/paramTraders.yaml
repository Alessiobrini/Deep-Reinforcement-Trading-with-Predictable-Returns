# The receipt is: - choose the parameters you want for the experiment - eventually choose the
# varying parameter - choose outputModel name to recognize different experiments
##########################################################################################################
# Parameters for the DQN implementation
epsilon : 1  # greedy policy choice
steps_to_min_eps : 350000 # decay for the epsilon parameter 0.1 linearly lead to 0.1n after 10 iter. 
min_eps : 0.1 # minimum possible value for epsilon
gamma : 0.9 # discounting factor for the Q target (sort of interest rate)
kappa : 0.0001 # risk aversion
std_rwds: [True,False]
##########################################################################################################
# NETWORK PARAMETERS
DQN_type : 'DDQN'
use_PER : False
PER_e : 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken
PER_a : 0.4  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly
PER_b : 0.6   # importance-sampling, from initial value increasing to 1
PER_b_anneal : True
final_PER_b : 1.0
PER_b_steps : 750000
PER_a_anneal : False
final_PER_a : 1.0
PER_a_steps : 750000
selected_loss: 'huber'
activation : 'elu'
kernel_initializer : 'he_uniform'
batch_norm_input : True # bool to decide if add batch norm to input layer or not
batch_norm_hidden : False
mom_batch_norm : 0.99
trainable_batch_norm : True
clipgrad : ''
clipnorm : 1.0
clipvalue : 1.0
clipglob_steps: 50000
optimizer_name : 'adam'
optimizer_decay :  0.0
beta_1: 0.9
beta_2: 0.999
eps_opt: 0.0000001
hidden_units : [128,64]
batch_size : 256
max_experiences : 500000
min_experiences : 5000
copy_step : 1000
update_target : 'hard'
tau : 0.0
learning_rate : 0.001 # 0.0003 as suggested by Karphaty
lr_schedule : 'exponential'
exp_decay_steps : 200000
final_lr : 0.0001
KLM :  [50000,10000,200000]# [200000,40000,2000000]  # list with actions that each trade can take: from -K to +K lot sizes minimum size of tradable (equities/contracts) and discretization of holding for TabQ (-M,M) holding
RT : [800, 0.0001] # boundaries for returns and ticksize to define the discrete space of returns (only for TabQ)
tablr: 0.001 # learning rate for updating q table
##########################################################################################################
# PRE TRAINING PARAMETERS
do_pretrain: False
N_pretrain : None
lrate_schedule_pretrain : None
save_ckpt_pretrained_model : 10000
##########################################################################################################
# DATA SIMULATIONS PARAMETERS
HalfLife : [10] # ,[10,206],[10,206,500]] #[5] #[5,206,500] # half-life of mean reversion to compute speed of mean reversion for the 3 factors
f0 : [0] #,[0,0],[0,0,0]] #0 #[0,0,0] # initial values for simulating factors
f_param : [0.01] #,[0.01, 0.0231],[0.01, 0.0231, -0.04250]] #0.01 #[0.01, 0.0231, -0.04250] # regression parameters for factors [10.32,122.34,-205.59]
sigma  : 0.01 # return volatility
sigmaf : [0.4] #,[0.4,0.2],[0.4,0.2, 0.05]] # factors volatility
CostMultiplier: 0.01 # Cost multiplier to make the cost proportional to the risk as in Gar Ped (2013)
                          # It is chosen as the median across the estimate of CM for each asset
discount_rate: 0.02 # discount rate as in the paper
Startholding: 0.0
#########################################################################################################
# LAUNCH SIMULATIONS
start_train : 1000
seed : [250,249,251,248] # Seed for experiment reproducibility 
N_train : 1000000  # training steps (each step consists of one action-value update)
out_of_sample_test : True
N_test: 100000
plot_inputs : 0 # boolean for plotting generated ret and factors
executeDRL: 1 # boolean to decide if run DRL agent
executeRL: 1 # boolean to decide if run RL agent
executeGP: 1 # boolean to decide if run GP solution
save_results: 1 # boolean to decide if save dataframe of results
save_table: 1
plot_hist: 1 # boolean to decide if plot weight histogram distribution (reduce size of tb logs)
plot_steps_hist : 20000
plot_steps: 1000 # steps for diagnostics on tensorboard
pdist_steps: 5000
save_model: 1 # save trained tf model
save_ckpt_model: 500000
saveresultsNBs: 0
use_GPU : True
##########################################################################################################
# FOLDER PARAMETERS
runtype: 'multi' # single, parallel, multiparallel
# Parameter for storing results
outputDir: 'outputs'
outputClass: 'FullTest20200528'
outputModel: 'Test_std_targets' # choose model name as number of factors + varying parameter
varying_pars : ['std_rwds','seed']   # choose a parameter to vary while performing several different experiment
 # leave blank if you don't want to vary any parameter (no subfolder will be created) 
varying_type : 'combination'
##########################################################################################################





