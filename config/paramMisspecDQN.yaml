############################## EXPLORATION ##############################
epsilon : 1  # Initial exploration probability
min_eps_pct : 1.0 # number of steps to reach the minimum epsilon as a percentage of the totaL
min_eps : 0.5 # minimum value for epsilon
############################## PER BUFFER ##############################
use_PER : False # use PER in training
PER_e : 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken
PER_a : 0.6  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly
PER_b : 0.4   # importance-sampling, from initial value increasing to 1
PER_b_anneal : True
final_PER_b : 1.0 # final value of b after the annealing
PER_b_steps : null
PER_a_anneal : False
final_PER_a : 1.0 # final value of a after the annealing
PER_a_steps : null
sample_type : 'TDerror' # Type of prioritization 'TDerror', 'diffTDerror' or 'reward'
############################## RL RELATED ##############################
DQN_type : 'DDQN' # 'DQN' or 'DDQN'
recurrent_env : False # set up recurrent environment
gamma : 0.55 # discounting factor for the Q target
kappa : 0.001 # risk aversion
std_rwds: False # standardize rewards
max_exp_pct : 1.0 # size of buffer experience as a percentage of the tota iteration
copy_step : 1 # steps for target network update in DQN
update_target : 'soft' # 'hard' or 'soft'
tau : 0.001 # size of soft update
action_type: 'MV' # GP or MV for selecting action boundaries
qts : [0.001,0.999] # quantile to select action boundaries
MV_res : [False,True] # Decide which actions to take (trading quantity or MVresiduals)
inp_type: 'f' #ret or f
KLM :  [null, null, 100000] # list with actions that each trade can take: from 
                            #-K to +K lot sizes minimum size of tradable (equities/contracts) 
                            # and discretization of holding for TabQ (-M,M) holding
zero_action : True # include the zero action (hold) in the action space
min_n_actions : False # set up minimum number of actions (3) or more
side_only : False # set the action space so that only the side of the bet is captured by the algorithm
discretization : null # float in (0,1] to determine a level of discretization for the action space when side_only=True. Leave empty for no discretization
temp : 200.0 # temperature parameter for boltzmann equation
bcm : False # allow behavioral cloning module
bcm_scale : 0.1 #[0.001,0.0001]
RT : [0.050, 0.0001] # boundaries for returns and ticksize to define the discrete space of returns (only for TabQ)
tablr: 0.005 # learning rate for updating q table
############################## DL RELATED ##############################
selected_loss: 'huber' # 'mse' or 'huber'
activation : 'elu' # 'elu', 'relu6', 'leaky_relu' or every other activation as aliased in TF2
kernel_initializer : 'he_uniform' # every kind of activation as aliased in TF2
optimizer_name : 'adam' # 'sgd', 'sgdmom', 'sgdnest, 'adadelta', 'adamax', 'amsgrad', 'adam', 'nadam', 'rmsprop'
beta_1: 0.5 # first parameter for adaptive optimizers
beta_2: 0.75 # second parameter for adaptive optimizers
eps_opt: 0.1 # corrective parameter for adaptive optimizers
hidden_units : [128,64] # list of hidden layers size
batch_size : 256
learning_rate : 0.005 # initial learning rate
lr_schedule : 'exponential' # 'exponential', 'piecewise', 'inverse_time' or 'polynomial'
exp_decay_pct : 0.30 # decay steps as percentage of the total iterations
exp_decay_rate : 0.6 # decay rate
hidden_memory_units: # size of the hidden recurrent layer, if any
unfolding: 1 # timesteps for recurrence. Set as 1 if there is no recurrence
############################## REGULARIZATION ##############################
batch_norm_input : True # batch norm at input layer level
batch_norm_hidden : False # batch norm at hidden layer level
clipgrad : null # clip gradient. Choose 'norm', 'value', 'globnorm' or null for no clipping
clipnorm : 1.0 # limit to clip the norm
clipvalue : 1.0 # limit to clip the value
clipglob_steps: 50000 # steps for global clipping of the norm
############################## DATA SIMULATIONS PARAMETERS ##############################
datatype: 'garch' # 'garch','t_stud','t_stud_mfit', 'garch_mr'
factor_lb: [1] # lsit of lags for fitting autoregressive vairables of the return series
CostMultiplier: 0.001 # Cost multiplier to make the cost proportional to the risk
discount_rate: 0.0 # discount rate for time value of money (not gamma in DQN)
Startholding: 0.0 # initial portfolio holding
############################## data parameters for GARCH simulation ##############################
mean_process: 'AR' # choice for mean process ['AR', 'Constant']
lags_mean_process : 1 # lag or list of lags for the autoregressive component in the mean return
vol_process : 'GARCH' # choice for volatility process ['GARCH', 'TGARCH', 'EGARCH', 'Constant']
distr_noise :  'normal' # cheoice for noise distribution ['normal', 'studt', 'skewstud', 'ged']
############################## data parameters for GP like simulation ##############################
t_stud : False # Student's T noise in the simulation
HalfLife : [350] # list of Halflives of mean reversion
f0 : [0] # list initial values for simulating factors
f_param : [0.00535] # list of factor loadings
sigma  : 0.01 # return volatility
sigmaf : [0.2] # list of factor volatilities
uncorrelated : True # generate correlated or uncorrelated factors
degrees : 6 # degrees of freedom for Student's T
#########################################################################################################
# LAUNCH SIMULATIONS
episodes : 1
start_train : 100 # steps after which the training starts
training : 'online' # online or offline
seed : [23,12] # seed for the return simulation
seed_param :  # seed for GARCH type simulation to sample random values for paramters
N_train : 1000  # training iteration (each step consists of one action-value update)
len_series : 1000 # length of simulated series, if null the legth is N_train
plot_inputs : 0 # boolean for plotting generated ret and factors
executeDRL: 1 # boolean to decide if run DRL agent
executeRL: 1 # boolean to decide if run RL agent
executeGP: 1 # boolean to decide if run GP solution
save_results: 0 # boolean to decide if save dataframe of results
save_table: 0 # boolean to decide if save Q-table
plot_hist: 0 # boolean to decide if plot weight histogram distribution (reduce size of tb logs)
plot_steps_hist : 20000 # steps for plotting histograms on tensorboard
plot_steps: 1000 # steps for all the other plots on tensorboard
save_model: 1 # save trained tf model
save_ckpt_model: 5 # number of checkpoints to save during training
use_GPU : True
#########################################################################################################
# FOLDER PARAMETERS
outputDir: 'outputs' # main directory for the experiment
outputClass: 'DQN' # first subdirectory
outputModel: '20200331_misspectestpar8' # second subdirectory
num_rnd_search : 8 # number of cores to use when experiments are parallelized
varying_pars: ['seed'] # choose parameters to vary while performing several different experiment
varying_type : 'chunk' # type of parallelization. 'chunk' will parallelize experiments
                       # in chunks of size 'num_rnd_search'. 'random_search' draw randomly some
                       # parameter combination depeding on all 'varying_pars' combinations.
                       # 'combination' parallelize all possible combination without considering
                       # the limit of cores in 'num_rnd_search'.
##########################################################################################################
