# The receipt is: - choose the parameters you want for the experiment - eventually choose the
# varying parameter - choose outputModel name to recognize different experiments
##########################################################################################################
# Parameters for the DQN implementation
epsilon : 1  # greedy policy choice
eps_decay : 0.0001 # decay for the epsilon parameter 0.1 linearly lead to 0.1n after 10 iter. 
min_eps : 0.1 # minimum possible value for epsilon
alpha : 0.001 # discounting factor for the Q updating error (learning rate)
gamma : 0.999 # discounting factor for the Q target (sort of interest rate)
kappa : 0.0001 # risk aversion
# add here parameters for the network
activation : 'tanh'
kernel_initializer : 'glorot_uniform'
batch_norm : 1 # bool to decide if add batch norm to input layer or not
hidden_units : [5,5] 
batch_size : 3
max_experiences : 10000
min_experiences : 5
copy_step : 20
learning_rate : 0.0003 # as suggested by Karphaty
LotSize  : 100 # minimum size of tradable (equities/contracts)
K  : 500  # actions that each trade can take: from -K to +K lot sizes
##########################################################################################################
# Parameters for simulations
HalfLife : [5] #[5] #[5,206,500] # half-life of mean reversion to compute speed of mean reversion for the 3 factors
f0 : [0] #0 #[0,0,0] # initial values for simulating factors
f_param : [0.01] #0.01 #[0.01, 0.0231, -0.04250] # regression parameters for factors [10.32,122.34,-205.59]
sigma  : 0.01 # return volatility
sigmaf : [0.4] # factors volatility
CostMultiplier: 0.01 #0.01 # Cost multiplier to make the cost proportional to the risk as in Gar Ped (2013)
                          # It is chosen as the median across the estimate of CM for each asset
discount_rate: 0.02 # discount rate as in the paper
Startholding: 0.0
#########################################################################################################
# Parameters for launching simulations
Seed: 42 # Seed for experiment reproducibility
N_train : 1000  # training steps (each step consists of one action-value update)
plot_inputs : 0 # boolean for plotting generated ret and factors
plot_insample: 0 # boolean for plotting intememdiate runtime results
executeRL: 1 # boolean to decide if run RL agent
execute_GP: 0 # boolean to decide if run GP solution
save_results: 0 # boolean to decide if save dataframe of results
plot_steps: 1000 # steps for plot of weights distribution on tensorboard
##########################################################################################################
# Parameter for storing results
outputDir: 'outputs'
outputClass: 'DQN'
outputModel: 'test' # choose model name as number of factors + varying parameter
varying_par :   # choose a parameter to vary while performing several different experiment
# leave blank if you don't want to vary any parameter (no subfolder will be created) 
##########################################################################################################

