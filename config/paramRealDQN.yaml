# # DQN PARAMS FOR PRETRAINING
do_pretrain: True
N_pretrain: [10000, 25000, 50000]
lr_schedule_pt: 'exponential'
save_ckpt_pretrained_model : 4
uncorrelated : True
plot_inputs : False
##########################################################################################################
# EXPLORATION
epsilon : 1  # greedy policy choice
# steps_to_min_eps : 3000  # decay for the epsilon parameter 0.1 linearly lead to 0.1n after 10 iter. 
min_eps : 0.0 # minimum possible value for epsilon
optimal_expl: False
alpha: 0.5
# PER BUFFER
use_PER : False
PER_e : 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken
PER_a : 0.4  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly
PER_b : 0.6   # importance-sampling, from initial value increasing to 1
PER_b_anneal : True
final_PER_b : 1.0
PER_b_steps : 125000
PER_a_anneal : False
final_PER_a : 1.0
PER_a_steps : 12000
# RL RELATED
DQN_type : 'DDQN'
recurrent_env : False
gamma : 0.55 # discounting factor for the Q target (sort of interest rate)
kappa : 0.001 # risk aversion
std_rwds: False
copy_step : 1
update_target : 'soft'
tau : 0.001
KLM :  [500, 250, 200000] # [200000,40000,2000000]  # list with actions that each trade can take: from -K to +K lot sizes minimum size of tradable (equities/contracts) and discretization of holding for TabQ (-M,M) holding
zero_action : True
RT : [500, 0.0001] # boundaries for returns and ticksize to define the discrete space of returns (only for TabQ)
tablr: 0.001 # learning rate for updating q table
# DL RELATED
selected_loss: 'huber'
activation : 'elu'
kernel_initializer : 'he_uniform'
optimizer_name : 'adam'
beta_1: 0.5
beta_2: 0.75
eps_opt: 0.1
hidden_units : [256, 128]
batch_size : 256
learning_rate : 0.005 # 0.0003 as suggested by Karphaty
lr_schedule : ''
exp_decay_pct : 0.15
exp_decay_rate : 0.6
hidden_memory_units:  
unfolding: 5
# REGULARIZATION
batch_norm_input : True # bool to decide if add batch norm to input layer or not
batch_norm_hidden : False
clipgrad : ''
clipnorm : 1.0
clipvalue : 1.0
clipglob_steps: 50000
##########################################################################################################
# DATA PARAMETERS
symbol: 'XG'
factor_lb: [5,252,1260]
CostMultiplier: 0.001 # Cost multiplier to make the cost proportional to the risk as in Gar Ped (2013)
                          # It is chosen as the median across the estimate of CM for each asset
discount_rate: 0.0 # discount rate as in the paper
Startholding: 0.0
#########################################################################################################
# LAUNCH SIMULATIONS
start_train : 300
seed_init : [758, 523, 339, 326, 881] # Seed for experiment reproducibility 
out_of_sample_test : False
executeDRL: 1 # boolean to decide if run DRL agent
executeRL: 0 # boolean to decide if run RL agent
executeGP: 1 # boolean to decide if run GP solution
executeMV: 0 # boolean to decide if run MV solution
save_results: 1 # boolean to decide if save dataframe of results
save_table: 1
plot_hist: 0 # boolean to decide if plot weight histogram distribution (reduce size of tb logs)
plot_steps_hist : 20000
plot_steps: 1000 # steps for diagnostics on tensorboard
pdist_steps: 5000
save_model: 1 # save trained tf model
save_ckpt_model: 4
use_GPU : False
#########################################################################################################
# FOLDER PARAMETERS
runtype: 'multi' # single, multi
# Parameter for storing results
outputDir: 'outputs'
outputClass: 'DQNreal_20201002'
outputModel: 'pt_multiseeds' # choose model name as number of factors + varying parameter
varying_pars: ['N_pretrain','seed_init'] #['HalfLife', 'f0', 'f_param', 'sigmaf'] # choose a parameter to vary while performing several different experiment
 # leave blank if you don't want to vary any parameter (no subfolder will be created) #['HalfLife','f0', 'f_param', 'sigmaf']
varying_type : 'combination'
num_rnd_search : 5
##########################################################################################################
