import utils.env
import utils.spaces
import utils.simulation
import agents.DQN
import agents.PPO

# Macros:
# ==============================================================================
# Set up
ALGO = 'PPO' # 'PPO', 'DQN' or 'DDPG'  

ENV_CLS = @MarketEnv #ShortMultiAssetCash


EXPERIMENT_TYPE = 'GP' # GP or Misspec
MV_RES = False # Decide which actions to take (trading quantity or MVresiduals)
MV_PENALTY = False #[True,False]
MV_PENALTY_COEF = None #1e-4 #[5e-2,1e-2,5e-3,1e-3,5e-4,1e-4,5e-5,1e-5,5e-6,1e-6,5e-7,1e-7,5e-8,1e-8,5e-9,1e-9,5e-10,1e-10,5e-11,1e-11]
UNIVERSAL_TRAIN = True #True

EPISODES = 6000 #None # episode to run the algorithm. Put None if you want to train DQN online
N_TRAIN = None
LEN_SERIES = 1200 #None # length of simulated series, if null the legth is N_train
EPOCHS = 3
OUTPUTDIR = 'outputs' # main directory for the experiment
OUTPUTCLASS = %ALGO # first subdirectory
OUTPUTMODEL = '20230415_GP'
SAVE_FREQ = 1000 #1000 #200  # number of checkpoints to save during training
SEED = 22 # [885]
START_TRAIN = 100 # steps after which the training starts
USE_GPU = True
VARYING_PARS = None #['%SEED'] 
VARYING_TYPE = 'chunk'
NUM_CORES = 1

# spaces
ACTION_RANGE =  [[-5e+5,5e+5], 9] # action range for standard RL [boundary,number of actions]
ACTION_RANGE_RES = [0.0, 1.0, 10] #[[0.0, 1.0, 10],[0.25, 1.0, 10],[0.5, 1.0, 10],[-0.25, 1.0, 10],[-0.5, 1.0, 10],[-1.0, 1.0, 10],[-1.5, 1.0, 10],[-2.0, 1.0, 10],[-2.5, 1.0, 10],[-3.0, 1.0, 10], [-3.5, 1.0, 10],[-4.0, 1.0, 10]] #[0.0, 1.0, 10] #[[-0.5, 1.0, 10],[-1.0, 1.0, 10],[0.0, 1.0, 10],[0.5, 1.0, 10]] # action range for residual RL [LB,UB,number of actions]
SIDE_ONLY = False # set the action space so that only the side of the bet is captured by the algorithm
ZERO_ACTION = True # include the zero action (hold) in the action space

# env and data simulation
DATATYPE = 'gauss' # type of data to simulate # 'garch','t_stud','t_stud_mfit', 'garch_mr', 'alpha_term_structure'
TIME_DEPENDENT = True #[False,True]

FACTOR_LB = [50] # list of lags for fitting autoregressive vairables of the return series
COSTMULTIPLIER = 1.0 #[1e-9,0.01,1.0,2.0,5.0] # Cost multiplier to make the cost proportional to the risk
CM1 = 0.0 #[0.01, 0.05, 0.005] #[0.01,0.001, 0.0005, 0.005] #[0.0005,0.001,0.0001,0.00005,0.00001] #[2.89E-4, 0.0]  
CM2 = 9E-4 #7.91E-4
STARTHOLDING = 0.0 # initial portfolio holding
DISCOUNT_RATE = 0.0 # discount rate for time value of money (not gamma in DQN)
KAPPA =  1E-2 #[1E-1,5E-1,1E-2,5E-2,1E-3,5E-3,1E-4,5E-4,1E-5,5E-5] #0.01 #[0.001,0.005] # risk aversion
INP_TYPE = 'ret' #['f','ret'] # 'f', 'ret', 'alpha' or 'alpha_f'
COST_TYPE = 'quadratic' #['nondiff','quadratic'] # quadratic or nondiff
DAILY_PRICE = 40
DAILY_VOLUME = 1E+5
REWARD_TYPE = 'mean_var' #['cara','mean_var'] # 'mean_var' 'cara'
CASH = None #1e+6

STOCHASTIC_POLICY = True
DT = 1.0 #[1.0,0.8, 0.75, 0.5, 0.25]
ROLLOUT_PCT_NUM = 1.0
MULTIASSET = False
N_ASSETS = 1
FIXED_ALPHA = True
HALFLIFE = [240] #[[120],[240]] #[240] # list of Halflives of mean reversion
INITIAL_ALPHA = [0.003] # only for alpha term structure case
SIGMA = 0.002 # return volatility
SIGMAF = [0.1] #[[0.1],[0.075],[0.05],[0.01],[0.025]] #[[0.1],[0.01]] #[[None],[1e-07], [5e-07], [1e-06], [5e-06], [1e-05], [2e-05],[3e-05],[4e-05], [5e-05],[7.5e-05],[1e-04],[2e-04],[5e-04],[10e-04]] # [[0.2, 0.1] ,[0.1, 0.1]] #[[0.2, 0.1],[0.1, 0.1]] # list of factor volatilities 
F_PARAM = [0.025] #, 0.1] # list of factor loadings
CORRELATION = None
DOUBLE_NOISE = True
QTS = [0.0, 1.0, 1.0] # quantile to select action boundaries print([[0.0,1.0,np.round(i,3)] for i in np.linspace(1.0,5.0,20)])
ACTION_TYPE = 'GP' # GP or MV for selecting action boundaries
UNCORRELATED = True # generate correlated or uncorrelated factors
T_STUD = False # Student's T noise in the simulation for GP factors
DEGREES = 6 # degrees of freedom for Student's T
VOL = 'omosk' #'omosk' or 'eterosk' to simulate GP-like return with stoch vol

MEAN_PROCESS = 'AR' # choice for mean process ['AR', 'Constant']
LAGS_MEAN_PROCESS = 1 # lag or list of lags for the autoregressive component in the mean return
VOL_PROCESS = 'GARCH' # choice for volatility process ['GARCH', 'TGARCH', 'EGARCH', 'Constant']
DISTR_NOISE = 'normal' # choice for noise distribution ['normal', 'studt', 'skewstud', 'ged']
P_ARG = [[0.075,0.0067,0.60],[0.075,0.0067,0.65],[0.075,0.0067,0.70],[0.075,0.0067,0.75],[0.075,0.0067,0.80],[0.075,0.0067,0.85]] # 3 values for garch parameters
# [[0.075,0.005,0.85],[0.075,0.007,0.85],[0.075,0.009,0.85],[0.075,0.02,0.85],[0.075,0.04,0.85],[0.075,0.06,0.85],[0.075,0.08,0.85],[0.075,0.01,0.85]]



# Parameters for main_runner:
# ==============================================================================
main_runner.algo = %ALGO


# Parameters for DQN_runner:
# ==============================================================================
DQN_runner.env_cls = %ENV_CLS
DQN_runner.MV_res = %MV_RES
DQN_runner.experiment_type = %EXPERIMENT_TYPE
DQN_runner.episodes = %EPISODES
DQN_runner.start_train = %START_TRAIN
DQN_runner.outputClass = %OUTPUTCLASS
DQN_runner.outputDir = %OUTPUTDIR
DQN_runner.outputModel = %OUTPUTMODEL
DQN_runner.save_freq = %SAVE_FREQ
DQN_runner.seed = %SEED
DQN_runner.use_GPU = %USE_GPU
DQN_runner.varying_pars = %VARYING_PARS
DQN_runner.varying_type = %VARYING_TYPE
DQN_runner.num_cores = %NUM_CORES
DQN_runner.N_train = %N_TRAIN
DQN_runner.len_series = %LEN_SERIES
DQN_runner.dt = %DT


# Parameters for DQN:
# ==============================================================================
DQN.DQN_type = 'DDQN' # 'DQN' or 'DDQN'
DQN.PER_a = 0.6 # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly
DQN.PER_a_growth = False
DQN.PER_b = 0.4 # importance-sampling, from initial value increasing to 1
DQN.PER_b_growth = True
DQN.PER_e = 0.01 # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken
DQN.activation = 'elu' # 'elu', 'relu6', 'leaky_relu' or every other activation as aliased in TF2
DQN.batch_norm_hidden = False # batch norm at hidden layer level
DQN.batch_norm_input = True # batch norm at input layer level
DQN.batch_size = 250 # size of the batch for the update
DQN.beta_1 = 0.5 # first parameter for adaptive optimizers
DQN.beta_2 = 0.75 # second parameter for adaptive optimizers
DQN.copy_step = 1.0 # steps for target network update in DQN: 'hard' or 'soft'
DQN.eps_opt = 0.1 # corrective parameter for adaptive optimizers
DQN.epsilon = 1 # Initial exploration probability
DQN.exp_decay_rate = 0.6 # decay rate
DQN.exp_decay_pct = 0.3 # decay steps as percentage of the total iterations
DQN.final_PER_a = 1.0 # final value of b after the annealing
DQN.final_PER_b = 1.0 # final value of b after the annealing
DQN.gamma = 0.55 # discounting factor for the Q target
DQN.hidden_memory_units = None # presence of hidden layer in the architecture (to implement)
DQN.hidden_units = [256, 128] # list of hidden layers size
DQN.kernel_initializer = 'he_uniform' # every kind of activation as aliased in TF2
DQN.lr = 0.005 # initial learning rate
DQN.lr_schedule = 'exponential' # 'exponential', 'piecewise', 'inverse_time' or 'polynomial'
DQN.max_exp_pct = 1.0 # size of buffer experience as a percentage of the total iteration
DQN.min_eps = 0.5 # minimum value for epsilon
DQN.min_eps_pct = 1.0 # number of steps to reach the minimum epsilon as a percentage of the total
DQN.optimizer_name = 'adam'
DQN.sample_type = 'TDerror' # Type of prioritization 'TDerror', 'diffTDerror' or 'reward'
DQN.seed = %SEED
DQN.selected_loss = 'huber' # 'mse' or 'huber'
DQN.start_train = %START_TRAIN
DQN.tau = 0.001 # size of soft update
DQN.update_target = 'soft' # type of update 'hard' or 'soft'
DQN.use_PER = False # use PER in training



# Parameters for PPO_runner:
# ==============================================================================
PPO_runner.env_cls = %ENV_CLS
PPO_runner.MV_res = %MV_RES
PPO_runner.experiment_type = %EXPERIMENT_TYPE
PPO_runner.episodes = %EPISODES
PPO_runner.outputClass = %OUTPUTCLASS
PPO_runner.outputDir = %OUTPUTDIR
PPO_runner.outputModel = %OUTPUTMODEL
PPO_runner.save_freq = %SAVE_FREQ
PPO_runner.seed = %SEED
PPO_runner.use_GPU = %USE_GPU
PPO_runner.varying_pars = %VARYING_PARS
PPO_runner.varying_type = %VARYING_TYPE
PPO_runner.num_cores = %NUM_CORES
PPO_runner.len_series = %LEN_SERIES
PPO_runner.dt = %DT
PPO_runner.rollouts_pct_num = %ROLLOUT_PCT_NUM 
PPO_runner.epochs = %EPOCHS
PPO_runner.universal_train = %UNIVERSAL_TRAIN
PPO_runner.store_insample = True


# Parameters for PPO:
# ==============================================================================
PPO.seed = %SEED
PPO.gamma = 0.99 #0.8 # discounting factor for the Q target
PPO.tau = 0.98 # lambda in the original GAE paper
PPO.clip_param = 0.2  # clipping of objective function
PPO.vf_c = 0.5 # coefficient for the value error
PPO.ent_c = 0.0001 # coefficient for entropy
PPO.hidden_units_value = [20,20]
PPO.hidden_units_actor = [20,20] #[[30,30],[40,40],[100,100],[20,20,20],[20,20],[10,10],[5,5],[5],[10],[20],[30],[50],[100,100,100]] #[[20,20],[20],[10],[6],[4],[3],[2]] #[20,20] #[20,20]
PPO.batch_size =  500 #250
PPO.lr = 0.0003 # [0.0003,0.0006,0.0009,0.0001] #0.0003 # initial learning rate
PPO.activation = 'tanh'
PPO.batch_norm_input = True # batch norm at input layer level
PPO.batch_norm_value_out = False # normalize value function output
PPO.policy_type = 'continuous' #discrete or continuous
PPO.init_pol_std = 0.0 # initial policy std dev for stochasticity
PPO.min_pol_std = 0.003 # minimum policy std dev for stochasticity
PPO.std_transform = 'exp'
PPO.init_last_layers = 'normal'
PPO.optimizer_name = 'adam'
PPO.beta_1 = 0.9 # first parameter for adaptive optimizers
PPO.beta_2 = 0.999 # second parameter for adaptive optimizers
PPO.eps_opt = 1e-8 # corrective parameter for adaptive optimizers
PPO.lr_schedule = ''
PPO.exp_decay_rate = 0.5
PPO.step_size = 10000
PPO.store_diagnostics = False
PPO.augadv = False
PPO.eta1 = 0.01
PPO.eta2 = 0.05
PPO.action_clipping_type = 'tanh' #[,'tanh'] #['tanh','env','none'] # env or tanh
PPO.tanh_stretching = 1.0 #0.3 # [0.28,0.3,0.32] # [0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.0] # stretching the tanh
PPO.scale_reward = True
PPO.gaussian_clipping = 3.3 #[3.0 , 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0 ]



# Parameters for ActionSpace:
# ==============================================================================
ActionSpace.action_range = %ACTION_RANGE
ActionSpace.side_only = %SIDE_ONLY
ActionSpace.zero_action = %ZERO_ACTION

# Parameters for ResActionSpace:
# ==============================================================================
ResActionSpace.action_range = %ACTION_RANGE_RES
ResActionSpace.zero_action = %ZERO_ACTION
ResActionSpace.side_only = %SIDE_ONLY

# Parameters for get_action_boundaries:
# ==============================================================================
get_action_boundaries.env_type = %ENV_CLS
get_action_boundaries.HalfLife = %HALFLIFE
get_action_boundaries.Startholding = %STARTHOLDING
get_action_boundaries.sigma = %SIGMA
get_action_boundaries.CostMultiplier = %COSTMULTIPLIER 
get_action_boundaries.kappa = %KAPPA
get_action_boundaries.discount_rate = %DISCOUNT_RATE 
get_action_boundaries.f_param = %F_PARAM
get_action_boundaries.qts = %QTS
get_action_boundaries.action_type = %ACTION_TYPE

# Parameters for MarketEnv:
# ==============================================================================
MarketEnv.CostMultiplier = %COSTMULTIPLIER
MarketEnv.Startholding = %STARTHOLDING
MarketEnv.discount_rate = %DISCOUNT_RATE
MarketEnv.kappa = %KAPPA
MarketEnv.sigma = %SIGMA
MarketEnv.HalfLife = %HALFLIFE
MarketEnv.kappa = %KAPPA
MarketEnv.f_param = %F_PARAM
MarketEnv.inp_type = %INP_TYPE
MarketEnv.cost_type = %COST_TYPE
MarketEnv.cm1 = %CM1
MarketEnv.cm2 = %CM2
MarketEnv.reward_type = %REWARD_TYPE
MarketEnv.cash = %CASH
MarketEnv.multiasset = %MULTIASSET
MarketEnv.corr = %CORRELATION
MarketEnv.mv_penalty = %MV_PENALTY
MarketEnv.mv_penalty_coef = %MV_PENALTY_COEF
MarketEnv.daily_volume = %DAILY_VOLUME
MarketEnv.daily_price = %DAILY_PRICE
MarketEnv.time_dependent = %TIME_DEPENDENT


# Parameters for DataHandler and its related functions:
# ==============================================================================
DataHandler.datatype = %DATATYPE
DataHandler.factor_lb = %FACTOR_LB

return_sampler_GP.sigmaf = %SIGMAF
return_sampler_GP.f_param = %F_PARAM
return_sampler_GP.sigma = %SIGMA
return_sampler_GP.HalfLife = %HALFLIFE
return_sampler_GP.uncorrelated = %UNCORRELATED
return_sampler_GP.t_stud = %T_STUD
return_sampler_GP.degrees = %DEGREES
return_sampler_GP.vol = %VOL
return_sampler_GP.dt = %DT


return_sampler_garch.mean_process = %MEAN_PROCESS
return_sampler_garch.lags_mean_process = %LAGS_MEAN_PROCESS
return_sampler_garch.vol_process = %VOL_PROCESS
return_sampler_garch.distr_noise = %DISTR_NOISE
return_sampler_garch.seed = %SEED

alpha_term_structure_sampler.HalfLife = %HALFLIFE
alpha_term_structure_sampler.initial_alpha = %INITIAL_ALPHA
alpha_term_structure_sampler.f_param = %F_PARAM
alpha_term_structure_sampler.generate_plot = False
alpha_term_structure_sampler.sigma = %SIGMA
alpha_term_structure_sampler.sigmaf = %SIGMAF
alpha_term_structure_sampler.multiasset = %MULTIASSET
alpha_term_structure_sampler.double_noise = %DOUBLE_NOISE
alpha_term_structure_sampler.fixed_alpha = %FIXED_ALPHA 

# Parameters for Out_sample_vs_gp:
# ==============================================================================
Out_sample_vs_gp.N_test = %LEN_SERIES
Out_sample_vs_gp.rnd_state =  %SEED #3425657
Out_sample_vs_gp.stochastic_policy =  %STOCHASTIC_POLICY

