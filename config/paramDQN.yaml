# DQN PARAMS
############################## EXPLORATION ##############################
epsilon : 1  # Initial exploration probability
min_eps_pct : 1.0 # number of steps to reach the minimum epsilon as a percentage of the total
min_eps : 0.5 # minimum value for epsilon
############################## PER BUFFER ##############################
use_PER : False # use PER in training
PER_e : 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken
PER_a : 0.6  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly
PER_b : 0.4   # importance-sampling, from initial value increasing to 1
PER_b_anneal : True
final_PER_b : 1.0 # final value of b after the annealing
PER_b_steps : null
PER_a_anneal : False
final_PER_a : 1.0 # final value of a after the annealing
PER_a_steps : null
sample_type : 'TDerror' # Type of prioritization 'TDerror', 'diffTDerror' or 'reward'
############################## RL RELATED ##############################
DQN_type : 'DDQN' # 'DQN' or 'DDQN'
recurrent_env : False # set up recurrent environment
gamma : 0.55 # discounting factor for the Q target
kappa : 0.001 # risk aversion
std_rwds: False # standardize rewards
max_exp_pct : 1.0 # size of buffer experience as a percentage of the tota iteration
copy_step : 1 # steps for target network update in DQN
update_target : 'soft' # 'hard' or 'soft'
tau : 0.001 # size of soft update
qts : [0.001,0.999] # quantile to select action boundaries
KLM :  [null, null, 100000] # list with actions that each trade can take: from 
                            #-K to +K lot sizes minimum size of tradable (equities/contracts) 
                            # and discretization of holding for TabQ (-M,M) holding
zero_action : True # include the zero action (hold) in the action space
min_n_actions : False # set up minimum number of actions (3) or more
RT : [0.050, 0.0001] # boundaries for returns and ticksize to define the discrete space of returns (only for TabQ)
tablr: 0.005 # learning rate for updating q table
############################## DL RELATED ##############################
selected_loss: 'huber' # 'mse' or 'huber'
activation : 'elu' # 'elu', 'relu6', 'leaky_relu' or every other activation as aliased in TF2
kernel_initializer : 'he_uniform' # every kind of activation as aliased in TF2
optimizer_name : 'adam' # 'sgd', 'sgdmom', 'sgdnest, 'adadelta', 'adamax', 'amsgrad', 'adam', 'nadam', 'rmsprop'
beta_1: 0.5 # first parameter for adaptive optimizers
beta_2: 0.75 # second parameter for adaptive optimizers
eps_opt: 0.1 # corrective parameter for adaptive optimizers
hidden_units : [128,64] # list of hidden layers size
batch_size : 256
learning_rate : 0.005 # initial learning rate
lr_schedule : 'exponential' # 'exponential', 'piecewise', 'inverse_time' or 'polynomial'
exp_decay_pct : 0.30 # decay steps as percentage of the total iterations
exp_decay_rate : 0.6 # decay rate
hidden_memory_units: # size of the hidden recurrent layer, if any
unfolding: 1 # timesteps for recurrence. Set as 1 if there is no recurrence
############################## REGULARIZATION ##############################
batch_norm_input : True # batch norm at input layer level
batch_norm_hidden : False # batch norm at hidden layer level
clipgrad : null # clip gradient. Choose 'norm', 'value', 'globnorm' or null for no clipping
clipnorm : 1.0 # limit to clip the norm
clipvalue : 1.0 # limit to clip the value
clipglob_steps: 50000 # steps for global clipping of the norm
############################## DATA SIMULATIONS PARAMETERS ##############################
t_stud : False # Student's T noise in the simulation
HalfLife : [350] # list of Halflives of mean reversion
f0 : [0] # list initial values for simulating factors
f_param : [0.00535] # list of factor loadings
sigma  : 0.01 # return volatility
sigmaf : [0.2] # list of factor volatilities
uncorrelated : True # generate correlated or uncorrelated factors
CostMultiplier: 0.001 # Cost multiplier to make the cost proportional to the risk
discount_rate: 0.0 # discount rate for time value of money (not gamma in DQN)
Startholding: 0.0 # initial portfolio holding
############################### LAUNCH SIMULATIONS ##############################
start_train : 1000 # steps after which the training starts
seed_ret : 543 # seed for the return simulation
seed_init :  # seed for weights initialization. Leave empty if you want it to be equal to seed_ret 
N_train : 2000  # training iteration (each step consists of one action-value update)
out_of_sample_test : False # do out-of-sample test while training
N_test: 1000 # test iteration
plot_inputs : 0 # boolean for plotting generated ret and factors
executeDRL: 1 # boolean to decide if run DRL agent
executeRL: 1 # boolean to decide if run RL agent
executeGP: 1 # boolean to decide if run GP solution
executeMV: 0 # boolean to decide if run MV solution
save_results: 0 # boolean to decide if save dataframe of results
save_table: 0 # boolean to decide if save Q-table
plot_hist: 0 # boolean to decide if plot weight histogram distribution (reduce size of tb logs)
plot_steps_hist : 20000 # steps for plotting histograms on tensorboard
plot_steps: 1000 # steps for all the other plots on tensorboard
save_model: 1 # save trained tf model
save_ckpt_model: 25 # number of checkpoints to save during training
use_GPU : True
#########################################################################################################
# FOLDER PARAMETERS
outputDir: 'outputs' # main directory for the experiment
outputClass: 'test' # first subdirectory
outputModel: 'testmutltmiss3' # second subdirectory
num_rnd_search : 8 # number of cores to use when experiments are parallelized
varying_pars: #['seed_ret']  # choose parameters to vary while performing several different experiment
varying_type : 'chunk' # type of parallelization. 'chunk' will parallelize experiments
                       # in chunks of size 'num_rnd_search'. 'random_search' draw randomly some
                       # parameter combination depeding on all 'varying_pars' combinations.
                       # 'combination' parallelize all possible combination without considering
                       # the limit of cores in 'num_rnd_search'.
##########################################################################################################
