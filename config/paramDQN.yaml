# DQN PARAMS
############################## EXPLORATION ##############################
epsilon : 1  # greedy policy choice
min_eps_pct : 0.85 # decay for the epsilon parameter 0.1 linearly lead to 0.1n after 10 iter. 
min_eps : 0.1 # minimum possible value for epsilon
optimal_expl: False
alpha: 0.3
############################## PER BUFFER ##############################
use_PER : False
PER_e : 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken
PER_a : 0.4  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly
PER_b : 0.6   # importance-sampling, from initial value increasing to 1
PER_b_anneal : True
final_PER_b : 1.0
PER_b_steps : 125000
PER_a_anneal : False
final_PER_a : 1.0
PER_a_steps : 12000
############################## RL RELATED ##############################
DQN_type : 'DDQN'
recurrent_env : False
gamma : 0.55 # discounting factor for the Q target (sort of interest rate)
kappa : 0.001 # risk aversion
std_rwds: False
max_exp_pct : 0.55
copy_step : 1
update_target : 'soft'
tau : 0.001
KLM :  [50000, 25000, 500000] # [200000,40000,2000000]  # list with actions that each trade can take: from -K to +K lot sizes minimum size of tradable (equities/contracts) and discretization of holding for TabQ (-M,M) holding
zero_action : True
RT : [500, 0.0001] # boundaries for returns and ticksize to define the discrete space of returns (only for TabQ)
tablr: 0.005 # learning rate for updating q table
############################## DL RELATED ##############################
selected_loss: 'huber'
activation : 'elu'
kernel_initializer : 'he_uniform'
optimizer_name : 'adam'
beta_1: 0.5
beta_2: 0.75
eps_opt: 0.1
hidden_units : [128,64]
batch_size : 256
learning_rate : 0.005 #[0.005, 0.001, 0.0005, 0.0001, 0.00005] # 0.0003 as suggested by Karphaty
lr_schedule : 'exponential'
exp_decay_pct : 0.15
exp_decay_rate : 0.6
hidden_memory_units: 
unfolding: 5
############################## REGULARIZATION ##############################
batch_norm_input : True # bool to decide if add batch norm to input layer or not
batch_norm_hidden : False
clipgrad : ''
clipnorm : 1.0
clipvalue : 1.0
clipglob_steps: 50000
############################## PRE TRAINING PARAMETERS ##############################
do_pretrain: False
N_pretrain : 200000
lrate_schedule_pretrain : None
save_ckpt_pretrained_model : 4
############################## DATA SIMULATIONS PARAMETERS ##############################
HalfLife : [2.5, 126.0, 630.0]
f0 : [0,0,0] #,[0,0],[0,0,0]] #0 #[0,0,0] # initial values for simulating factors
f_param : [0.0214, 0.0231, -0.0225] #  #0.01 #[0.01, 0.0231, -0.04250] # regression parameters for factors [10.32,122.34,-205.59]
sigma  : 0.01 # return volatility
sigmaf : [0.2, 0.1, 0.05] #[0.2, 0.1, 0.05] # factors volatility
uncorrelated : True
hetersk : False
alpha : 0.4
beta : 0.6
CostMultiplier: 0.001 # Cost multiplier to make the cost proportional to the risk as in Gar Ped (2013)
discount_rate: 0.0 # discount rate as in the paper
Startholding: 0.0
############################### LAUNCH SIMULATIONS ##############################
start_train : 1000
seed_ret : [901,  24, 994, 475, 955, 690, 566, 682, 408, 573, 862, 133, 761,
       621, 797, 683]
seed_init :  # Seed for experiment reproducibility 
N_train : 100000  # training steps (each step consists of one action-value update)
out_of_sample_test : True
N_test: 10000
plot_inputs : 0 # boolean for plotting generated ret and factors
executeDRL: 1 # boolean to decide if run DRL agent
executeRL: 1 # boolean to decide if run RL agent
executeGP: 1 # boolean to decide if run GP solution
executeMV: 1 # boolean to decide if run MV solution
save_results: 1 # boolean to decide if save dataframe of results
save_table: 1
plot_hist: 0 # boolean to decide if plot weight histogram distribution (reduce size of tb logs)
plot_steps_hist : 20000
plot_steps: 1000 # steps for diagnostics on tensorboard
pdist_steps: 5000
save_model: 1 # save trained tf model
save_ckpt_model: 8
use_GPU : False
#########################################################################################################
# FOLDER PARAMETERS
runtype: 'multi' # single, multi
# Parameter for storing results
outputDir: 'outputs'
outputClass: 'DQN'
outputModel: '20201103_seeds' # choose model name as number of factors + varying parameter
varying_pars: ['seed_ret'] #['HalfLife', 'f0', 'f_param', 'sigmaf'] # choose a parameter to vary while performing several different experiment
 # leave blank if you don't want to vary any parameter (no subfolder will be created) #['HalfLife','f0', 'f_param', 'sigmaf']
varying_type : 'combination'
num_rnd_search : 30
##########################################################################################################
