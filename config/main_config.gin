import utils.env
import utils.spaces
import utils.simulation
import agents.DQN
import agents.PPO

# Macros:
# ==============================================================================
# Set up
ALGO = 'PPO' # 'PPO', 'DQN' or 'DDPG'  

ENV_CLS = @MarketEnv 


EXPERIMENT_TYPE = 'GP' # GP or Misspec
MV_RES = True  # Decide which actions to take (trading quantity or MVresiduals)
UNIVERSAL_TRAIN = True

EPISODES = 10  #None # episode to run the algorithm. Put None if you want to train DQN online
N_TRAIN = None
LEN_SERIES = 500  #None # length of simulated series, if null the legth is N_train
EPOCHS = 3
OUTPUTDIR = 'outputs' # main directory for the experiment
OUTPUTCLASS = %ALGO # first subdirectory
OUTPUTMODEL = '20210603_test4' # second subdirectory
SAVE_FREQ = 5  # number of checkpoints to save during training
SEED =  11 #[297, 932, 165,  26]  # seed for reproducibility
START_TRAIN = 300 # steps after which the training starts
USE_GPU = True
VARYING_PARS =  None #['%SEED']
VARYING_TYPE = 'chunk'
NUM_CORES = 48

# spaces
ACTION_RANGE =  [None, 9] # action range for standard RL [boundary,number of actions]
ACTION_RANGE_RES =  [-0.5, 1.0, 10] # action range for residual RL [LB,UB,number of actions]
SIDE_ONLY = False # set the action space so that only the side of the bet is captured by the algorithm
ZERO_ACTION = True # include the zero action (hold) in the action space

#discretization :  # float in (0,1] to determine a level of discretization for the action space when side_only=True. Leave empty for no discretization
#temp : null #[10.0, 50.0, 100.0, 200.0] # temperature parameter for boltzmann equation
#bcm : False # allow behavioral cloning module
#bcm_scale :  #[0.001,0.0001]


# env and data simulation
DATATYPE = 'alpha_term_structure' # type of data to simulate # 'garch','t_stud','t_stud_mfit', 'garch_mr', 'alpha_term_structure'

FACTOR_LB = [2, 50] # list of lags for fitting autoregressive vairables of the return series
COSTMULTIPLIER = 0.01 # Cost multiplier to make the cost proportional to the risk
CM1 = 2.89E-4  
CM2 = 7.91E-4
STARTHOLDING = 0.0 # initial portfolio holding
DISCOUNT_RATE = 0.0 # discount rate for time value of money (not gamma in DQN)
KAPPA = 0.001 # risk aversion
INP_TYPE = 'alpha' #['f','ret'] # 'f', 'ret' or 'alpha'
COST_TYPE = 'nondiff' # quadratic or nondiff

DT = 1.0
ROLLOUT_PCT_NUM = 1.0
HALFLIFE = [5,15, 45] # [[2.5,11],[2.5,126],[66,630]] # list of Halflives of mean reversion
INITIAL_ALPHA = [0.035, -0.045, 0.012] # only for alpha term structure case
SIGMA = 0.01 # return volatility
SIGMAF = [0.0000005] #[[0.2, 0.1] ,[0.1, 0.1] ] #[[0.2, 0.1],[0.1, 0.1]] # list of factor volatilities
F_PARAM = [0.005] #, 0.005] # list of factor loadings
QTS = [0.001,0.999] # quantile to select action boundaries
ACTION_TYPE = 'MV' # GP or MV for selecting action boundaries
UNCORRELATED = True # generate correlated or uncorrelated factors
T_STUD = False # Student's T noise in the simulation for GP factors
DEGREES = 6 # degrees of freedom for Student's T
VOL = 'omosk' #'omosk' or 'eterosk' to simulate GP-like return with stoch vol

MEAN_PROCESS = 'AR' # choice for mean process ['AR', 'Constant']
LAGS_MEAN_PROCESS = 1 # lag or list of lags for the autoregressive component in the mean return
VOL_PROCESS = 'GARCH' # choice for volatility process ['GARCH', 'TGARCH', 'EGARCH', 'Constant']
DISTR_NOISE = 'normal' # choice for noise distribution ['normal', 'studt', 'skewstud', 'ged']



# Parameters for main_runner:
# ==============================================================================
main_runner.algo = %ALGO


# Parameters for DQN_runner:
# ==============================================================================
DQN_runner.env_cls = %ENV_CLS
DQN_runner.MV_res = %MV_RES
DQN_runner.experiment_type = %EXPERIMENT_TYPE
DQN_runner.episodes = %EPISODES
DQN_runner.start_train = %START_TRAIN
DQN_runner.outputClass = %OUTPUTCLASS
DQN_runner.outputDir = %OUTPUTDIR
DQN_runner.outputModel = %OUTPUTMODEL
DQN_runner.save_freq = %SAVE_FREQ
DQN_runner.seed = %SEED
DQN_runner.use_GPU = %USE_GPU
DQN_runner.varying_pars = %VARYING_PARS
DQN_runner.varying_type = %VARYING_TYPE
DQN_runner.num_cores = %NUM_CORES
DQN_runner.N_train = %N_TRAIN
DQN_runner.len_series = %LEN_SERIES
DQN_runner.dt = %DT


# Parameters for DQN:
# ==============================================================================
DQN.DQN_type = 'DDQN' # 'DQN' or 'DDQN'
DQN.PER_a = 0.6 # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly
DQN.PER_a_growth = False
DQN.PER_b = 0.4 # importance-sampling, from initial value increasing to 1
DQN.PER_b_growth = True
DQN.PER_e = 0.01 # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken
DQN.activation = 'elu' # 'elu', 'relu6', 'leaky_relu' or every other activation as aliased in TF2
DQN.batch_norm_hidden = False # batch norm at hidden layer level
DQN.batch_norm_input = True # batch norm at input layer level
DQN.batch_size = 250 # size of the batch for the update
DQN.beta_1 = 0.5 # first parameter for adaptive optimizers
DQN.beta_2 = 0.75 # second parameter for adaptive optimizers
DQN.copy_step = 1.0 # steps for target network update in DQN: 'hard' or 'soft'
DQN.eps_opt = 0.1 # corrective parameter for adaptive optimizers
DQN.epsilon = 1 # Initial exploration probability
DQN.exp_decay_rate = 0.6 # decay rate
DQN.exp_decay_pct = 0.3 # decay steps as percentage of the total iterations
DQN.final_PER_a = 1.0 # final value of b after the annealing
DQN.final_PER_b = 1.0 # final value of b after the annealing
DQN.gamma = 0.55 # discounting factor for the Q target
DQN.hidden_memory_units = None # presence of hidden layer in the architecture (to implement)
DQN.hidden_units = [256, 128] # list of hidden layers size
DQN.kernel_initializer = 'he_uniform' # every kind of activation as aliased in TF2
DQN.lr = 0.005 # initial learning rate
DQN.lr_schedule = 'exponential' # 'exponential', 'piecewise', 'inverse_time' or 'polynomial'
DQN.max_exp_pct = 1.0 # size of buffer experience as a percentage of the total iteration
DQN.min_eps = 0.5 # minimum value for epsilon
DQN.min_eps_pct = 1.0 # number of steps to reach the minimum epsilon as a percentage of the total
DQN.optimizer_name = 'adam'
DQN.sample_type = 'TDerror' # Type of prioritization 'TDerror', 'diffTDerror' or 'reward'
DQN.seed = %SEED
DQN.selected_loss = 'huber' # 'mse' or 'huber'
DQN.start_train = %START_TRAIN
DQN.tau = 0.001 # size of soft update
DQN.update_target = 'soft' # type of update 'hard' or 'soft'
DQN.use_PER = False # use PER in training



# Parameters for PPO_runner:
# ==============================================================================
PPO_runner.env_cls = %ENV_CLS
PPO_runner.MV_res = %MV_RES
PPO_runner.experiment_type = %EXPERIMENT_TYPE
PPO_runner.episodes = %EPISODES
PPO_runner.outputClass = %OUTPUTCLASS
PPO_runner.outputDir = %OUTPUTDIR
PPO_runner.outputModel = %OUTPUTMODEL
PPO_runner.save_freq = %SAVE_FREQ
PPO_runner.seed = %SEED
PPO_runner.use_GPU = %USE_GPU
PPO_runner.varying_pars = %VARYING_PARS
PPO_runner.varying_type = %VARYING_TYPE
PPO_runner.num_cores = %NUM_CORES
PPO_runner.len_series = %LEN_SERIES
PPO_runner.dt = %DT
PPO_runner.rollouts_pct_num = %ROLLOUT_PCT_NUM 
PPO_runner.epochs = %EPOCHS
PPO_runner.universal_train = %UNIVERSAL_TRAIN

# Parameters for PPO:
# ==============================================================================
PPO.seed = %SEED
PPO.gamma = 0.99 #0.8 # discounting factor for the Q target
PPO.tau = 0.98 # lambda in the original GAE paper
PPO.clip_param = 0.3 # clipping of objective function
PPO.vf_c = 0.5 # coefficient for the value error
PPO.ent_c = 0.001 # coefficient for entropy
PPO.hidden_units_value = [256,128]
PPO.hidden_units_actor = [128,64]
PPO.batch_size =  250
PPO.lr = 0.0003 # initial learning rate
PPO.activation = 'tanh'
PPO.batch_norm_input = True # batch norm at input layer level
PPO.batch_norm_value_out = False # normalize value function output
PPO.policy_type = 'continuous' #discrete or continuous
PPO.init_pol_std = 0.5 # initial policy std dev for stochasticity
PPO.min_pol_std = 0.003 # minimum policy std dev for stochasticity
PPO.std_transform = 'exp'
PPO.init_last_layers = 'normal'
PPO.optimizer_name = 'adam'
PPO.beta_1 = 0.9 # first parameter for adaptive optimizers
PPO.beta_2 = 0.999 # second parameter for adaptive optimizers
PPO.eps_opt = 1e-8 # corrective parameter for adaptive optimizers
PPO.lr_schedule = ''
PPO.exp_decay_rate = 0.999
PPO.step_size = None



# Parameters for ActionSpace:
# ==============================================================================
ActionSpace.action_range = %ACTION_RANGE
ActionSpace.side_only = %SIDE_ONLY
ActionSpace.zero_action = %ZERO_ACTION

# Parameters for ResActionSpace:
# ==============================================================================
ResActionSpace.action_range = %ACTION_RANGE_RES
ResActionSpace.zero_action = %ZERO_ACTION
ResActionSpace.side_only = %SIDE_ONLY

# Parameters for get_action_boundaries:
# ==============================================================================
get_action_boundaries.HalfLife = %HALFLIFE
get_action_boundaries.Startholding = %STARTHOLDING
get_action_boundaries.sigma = %SIGMA
get_action_boundaries.CostMultiplier = %COSTMULTIPLIER 
get_action_boundaries.kappa = %KAPPA
get_action_boundaries.discount_rate = %DISCOUNT_RATE 
get_action_boundaries.f_param = %F_PARAM
get_action_boundaries.qts = %QTS
get_action_boundaries.action_type = %ACTION_TYPE

# Parameters for MarketEnv:
# ==============================================================================
MarketEnv.CostMultiplier = %COSTMULTIPLIER
MarketEnv.Startholding = %STARTHOLDING
MarketEnv.discount_rate = %DISCOUNT_RATE
MarketEnv.kappa = %KAPPA
MarketEnv.sigma = %SIGMA
MarketEnv.HalfLife = %HALFLIFE
MarketEnv.kappa = %KAPPA
MarketEnv.f_param = %F_PARAM
MarketEnv.inp_type = %INP_TYPE
MarketEnv.cost_type = %COST_TYPE
MarketEnv.cm1 = %CM1
MarketEnv.cm2 = %CM2


# Parameters for DataHandler and its related functions:
# ==============================================================================
DataHandler.datatype = %DATATYPE
DataHandler.factor_lb = %FACTOR_LB

return_sampler_GP.sigmaf = %SIGMAF
return_sampler_GP.f_param = %F_PARAM
return_sampler_GP.sigma = %SIGMA
return_sampler_GP.HalfLife = %HALFLIFE
return_sampler_GP.uncorrelated = %UNCORRELATED
return_sampler_GP.t_stud = %T_STUD
return_sampler_GP.degrees = %DEGREES
return_sampler_GP.vol = %VOL
return_sampler_GP.dt = %DT


return_sampler_garch.mean_process = %MEAN_PROCESS
return_sampler_garch.lags_mean_process = %LAGS_MEAN_PROCESS
return_sampler_garch.vol_process = %VOL_PROCESS
return_sampler_garch.distr_noise = %DISTR_NOISE
return_sampler_garch.seed = %SEED

alpha_term_structure_sampler.HalfLife = %HALFLIFE
alpha_term_structure_sampler.initial_alpha = %INITIAL_ALPHA
alpha_term_structure_sampler.generate_plot = False

# Parameters for Out_sample_vs_gp:
# ==============================================================================
Out_sample_vs_gp.n_seeds = 3
Out_sample_vs_gp.N_test = 1000
Out_sample_vs_gp.rnd_state =  3425657

