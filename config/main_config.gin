import utils.env
import utils.spaces
import utils.simulation
import agents.DQN
import agents.PPO

# Macros:
# ==============================================================================
# Set up
ALGO = 'PPO' # 'PPO', 'DQN' or 'DDPG'  

ENV_CLS = @ShortMultiAssetCashMarketEnv 


EXPERIMENT_TYPE = 'GP' # GP or Misspec
MV_RES = True  # Decide which actions to take (trading quantity or MVresiduals)
UNIVERSAL_TRAIN = True

EPISODES = 200  #None # episode to run the algorithm. Put None if you want to train DQN online
N_TRAIN = None
LEN_SERIES = 200 #None # length of simulated series, if null the legth is N_train
EPOCHS = 3
OUTPUTDIR = 'outputs' # main directory for the experiment
OUTPUTCLASS = %ALGO # first subdirectory
OUTPUTMODEL = '20210802_test2' # second subdirectory
SAVE_FREQ = 10  # number of checkpoints to save during training
SEED = 5465 #[765,  35, 654, 850] #[157, 466, 477,  24, 281]  # seed for reproducibility
START_TRAIN = 300 # steps after which the training starts
USE_GPU = True
VARYING_PARS = None #['%INITIAL_ALPHA', '%CORRELATION', '%SEED']
VARYING_TYPE = 'chunk'
NUM_CORES = 60

# spaces
ACTION_RANGE =  [None, 9] # action range for standard RL [boundary,number of actions]
ACTION_RANGE_RES = [0.0, 1.0, 10] #[[-0.5, 1.0, 10],[-1.0, 1.0, 10],[0.0, 1.0, 10],[0.5, 1.0, 10]] # action range for residual RL [LB,UB,number of actions]
SIDE_ONLY = False # set the action space so that only the side of the bet is captured by the algorithm
ZERO_ACTION = True # include the zero action (hold) in the action space

#discretization :  # float in (0,1] to determine a level of discretization for the action space when side_only=True. Leave empty for no discretization
#temp : null #[10.0, 50.0, 100.0, 200.0] # temperature parameter for boltzmann equation
#bcm : False # allow behavioral cloning module
#bcm_scale :  #[0.001,0.0001]


# env and data simulation
DATATYPE = 'alpha_term_structure' # type of data to simulate # 'garch','t_stud','t_stud_mfit', 'garch_mr', 'alpha_term_structure'

FACTOR_LB = [2, 50] # list of lags for fitting autoregressive vairables of the return series
COSTMULTIPLIER = 0.001 #[0.0005,0.001,0.0001,0.00005,0.00001] # Cost multiplier to make the cost proportional to the risk
CM1 = 0.001 #[0.01, 0.05, 0.005] #[0.01,0.001, 0.0005, 0.005] #[0.0005,0.001,0.0001,0.00005,0.00001] #[2.89E-4, 0.0]  
CM2 = 0.001 #7.91E-4
STARTHOLDING = 0.0 # initial portfolio holding
DISCOUNT_RATE = 0.0 # discount rate for time value of money (not gamma in DQN)
KAPPA = 0.001 #[0.001,0.005] # risk aversion
INP_TYPE = 'alpha_f' #['f','ret'] # 'f', 'ret', 'alpha' or 'alpha_f'
COST_TYPE = 'quadratic' # quadratic or nondiff
REWARD_TYPE = 'mean_var' # 'mean_var' 'cara'
CASH = 1e+9

DT = 1.0
ROLLOUT_PCT_NUM = 1.0
MULTIASSET = True
N_ASSETS = 1000
HALFLIFE =  [[120], [106], [6], [137], [78], [149], [30], [131], [140], [137], [62], [141], [84], [52], [110], [149], [147], [101], [129], [16], [43], [20], [81], [79], [133]]
INITIAL_ALPHA =  [[0.0022], [0.00185], [0.00017], [0.00116], [0.00156], [0.00362], [0.00115], [0.00332], [0.00176], [0.00383], [0.00308], [0.00241], [0.00297], [0.00172], [0.00157], [0.00085], [0.00048], [0.00272], [0.0002], [0.00231], [0.00154], [0.00326], [0.00069], [0.00151], [0.00048]]
F_PARAM =  [[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]]
CORRELATION =  [0.31414, 0.68822, 0.84198, 0.28926, 0.98277, 0.30317, 0.6545, 0.42945, 0.3995, 0.66423, 0.14525, 0.70554, 0.01606, 0.68693, 0.55504, 0.00445, 0.2565, 0.53978, 0.21587, 0.92101, 0.01421, 0.54514, 0.47327, 0.07249, 0.54215, 0.54873, 0.23365, 0.91547, 0.30883, 0.48249, 0.7284, 0.25157, 0.89107, 0.22601, 0.39968, 0.28956, 0.33893, 0.82441, 0.67537, 0.93043, 0.67593, 0.08649, 0.86602, 0.84602, 0.14531, 0.2596, 0.6506, 0.51838, 0.4863, 0.25659, 0.26982, 0.40234, 0.22442, 0.46991, 0.58638, 0.82985, 0.24055, 0.42734, 0.88096, 0.21549, 0.51543, 0.36567, 0.62195, 0.25303, 0.90237, 0.78874, 0.48133, 0.58782, 0.3058, 0.55955, 0.56234, 0.92796, 0.77035, 0.67323, 0.76901, 0.67003, 0.42863, 0.93622, 0.90686, 0.85383, 0.18479, 0.81791, 0.80618, 0.26248, 0.12464, 0.74544, 0.05192, 0.11559, 0.83372, 0.10269, 0.03204, 0.02727, 0.09013, 0.9924, 0.67289, 0.73909, 0.14203, 0.26982, 0.54844, 0.16421, 0.04651, 0.30197, 0.69626, 0.07211, 0.24363, 0.55427, 0.76269, 0.95892, 0.32463, 0.41421, 0.74474, 0.41836, 0.98444, 0.05705, 0.62194, 0.84571, 0.41431, 0.73412, 0.26656, 0.04873, 0.52667, 0.8314, 0.03573, 0.73452, 0.94103, 0.91134, 0.95598, 0.01331, 0.43519, 0.60918, 0.22789, 0.21391, 0.40423, 0.06588, 0.03363, 0.56323, 0.71004, 0.56651, 0.19911, 0.04772, 0.62286, 0.25551, 0.57192, 0.68958, 0.25288, 0.04757, 0.81688, 0.50752, 0.95369, 0.51487, 0.15857, 0.36595, 0.71496, 0.47247, 0.68209, 0.33699, 0.96002, 0.13968, 0.32507, 0.1711, 0.93482, 0.71271, 0.95844, 0.66005, 0.98036, 0.31735, 0.0252, 0.12584, 0.24883, 0.59368, 0.41515, 0.2716, 0.95337, 0.96627, 0.33419, 0.32148, 0.34831, 0.91365, 0.83361, 0.30001, 0.10264, 0.14739, 0.50221, 0.25095, 0.67257, 0.85514, 0.4177, 0.99681, 0.48685, 0.57932, 0.67844, 0.31101, 0.87036, 0.9971, 0.64345, 0.28457, 0.05137, 0.63777, 0.69943, 0.31321, 0.75838, 0.91378, 0.28143, 0.34167, 0.293, 0.07226, 0.11347, 0.86542, 0.53072, 0.65428, 0.50557, 0.61929, 0.18538, 0.85745, 0.01158, 0.39188, 0.85865, 0.87026, 0.31367, 0.7511, 0.63331, 0.11529, 0.40414, 0.30613, 0.96462, 0.78534, 0.81158, 0.85446, 0.53734, 0.31254, 0.25956, 0.298, 0.02976, 0.09927, 0.13324, 0.30131, 0.19546, 0.77594, 0.64912, 0.17763, 0.17199, 0.83438, 0.37574, 0.40739, 0.62502, 0.48196, 0.57754, 0.14423, 0.19781, 0.69943, 0.91388, 0.55145, 0.64947, 0.91395, 0.14453, 0.97794, 0.55195, 0.25091, 0.56653, 0.79376, 0.56656, 0.02764, 0.89266, 0.90893, 0.58857, 0.4088, 0.2734, 0.77203, 0.48542, 0.43762, 0.11208, 0.38628, 0.59368, 0.7479, 0.53845, 0.61135, 0.47989, 0.9581, 0.97004, 0.48858, 0.39366, 0.05491, 0.25114, 0.64807, 0.15935, 0.32263, 0.45813, 0.24866, 0.75177, 0.17421, 0.74819, 0.47549, 0.28757, 0.40766, 0.65129, 0.82369, 0.67064, 0.65162, 0.47909, 0.93946]
SIGMA = 0.01 # return volatility
SIGMAF = [None] #[[0.2, 0.1] ,[0.1, 0.1]] #[[0.2, 0.1],[0.1, 0.1]] # list of factor volatilities
QTS = [0.001,0.999] # quantile to select action boundaries
ACTION_TYPE = 'MV' # GP or MV for selecting action boundaries
UNCORRELATED = True # generate correlated or uncorrelated factors
T_STUD = False # Student's T noise in the simulation for GP factors
DEGREES = 6 # degrees of freedom for Student's T
VOL = 'omosk' #'omosk' or 'eterosk' to simulate GP-like return with stoch vol

MEAN_PROCESS = 'AR' # choice for mean process ['AR', 'Constant']
LAGS_MEAN_PROCESS = 1 # lag or list of lags for the autoregressive component in the mean return
VOL_PROCESS = 'GARCH' # choice for volatility process ['GARCH', 'TGARCH', 'EGARCH', 'Constant']
DISTR_NOISE = 'normal' # choice for noise distribution ['normal', 'studt', 'skewstud', 'ged']



# Parameters for main_runner:
# ==============================================================================
main_runner.algo = %ALGO


# Parameters for DQN_runner:
# ==============================================================================
DQN_runner.env_cls = %ENV_CLS
DQN_runner.MV_res = %MV_RES
DQN_runner.experiment_type = %EXPERIMENT_TYPE
DQN_runner.episodes = %EPISODES
DQN_runner.start_train = %START_TRAIN
DQN_runner.outputClass = %OUTPUTCLASS
DQN_runner.outputDir = %OUTPUTDIR
DQN_runner.outputModel = %OUTPUTMODEL
DQN_runner.save_freq = %SAVE_FREQ
DQN_runner.seed = %SEED
DQN_runner.use_GPU = %USE_GPU
DQN_runner.varying_pars = %VARYING_PARS
DQN_runner.varying_type = %VARYING_TYPE
DQN_runner.num_cores = %NUM_CORES
DQN_runner.N_train = %N_TRAIN
DQN_runner.len_series = %LEN_SERIES
DQN_runner.dt = %DT


# Parameters for DQN:
# ==============================================================================
DQN.DQN_type = 'DDQN' # 'DQN' or 'DDQN'
DQN.PER_a = 0.6 # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly
DQN.PER_a_growth = False
DQN.PER_b = 0.4 # importance-sampling, from initial value increasing to 1
DQN.PER_b_growth = True
DQN.PER_e = 0.01 # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken
DQN.activation = 'elu' # 'elu', 'relu6', 'leaky_relu' or every other activation as aliased in TF2
DQN.batch_norm_hidden = False # batch norm at hidden layer level
DQN.batch_norm_input = True # batch norm at input layer level
DQN.batch_size = 250 # size of the batch for the update
DQN.beta_1 = 0.5 # first parameter for adaptive optimizers
DQN.beta_2 = 0.75 # second parameter for adaptive optimizers
DQN.copy_step = 1.0 # steps for target network update in DQN: 'hard' or 'soft'
DQN.eps_opt = 0.1 # corrective parameter for adaptive optimizers
DQN.epsilon = 1 # Initial exploration probability
DQN.exp_decay_rate = 0.6 # decay rate
DQN.exp_decay_pct = 0.3 # decay steps as percentage of the total iterations
DQN.final_PER_a = 1.0 # final value of b after the annealing
DQN.final_PER_b = 1.0 # final value of b after the annealing
DQN.gamma = 0.55 # discounting factor for the Q target
DQN.hidden_memory_units = None # presence of hidden layer in the architecture (to implement)
DQN.hidden_units = [256, 128] # list of hidden layers size
DQN.kernel_initializer = 'he_uniform' # every kind of activation as aliased in TF2
DQN.lr = 0.005 # initial learning rate
DQN.lr_schedule = 'exponential' # 'exponential', 'piecewise', 'inverse_time' or 'polynomial'
DQN.max_exp_pct = 1.0 # size of buffer experience as a percentage of the total iteration
DQN.min_eps = 0.5 # minimum value for epsilon
DQN.min_eps_pct = 1.0 # number of steps to reach the minimum epsilon as a percentage of the total
DQN.optimizer_name = 'adam'
DQN.sample_type = 'TDerror' # Type of prioritization 'TDerror', 'diffTDerror' or 'reward'
DQN.seed = %SEED
DQN.selected_loss = 'huber' # 'mse' or 'huber'
DQN.start_train = %START_TRAIN
DQN.tau = 0.001 # size of soft update
DQN.update_target = 'soft' # type of update 'hard' or 'soft'
DQN.use_PER = False # use PER in training



# Parameters for PPO_runner:
# ==============================================================================
PPO_runner.env_cls = %ENV_CLS
PPO_runner.MV_res = %MV_RES
PPO_runner.experiment_type = %EXPERIMENT_TYPE
PPO_runner.episodes = %EPISODES
PPO_runner.outputClass = %OUTPUTCLASS
PPO_runner.outputDir = %OUTPUTDIR
PPO_runner.outputModel = %OUTPUTMODEL
PPO_runner.save_freq = %SAVE_FREQ
PPO_runner.seed = %SEED
PPO_runner.use_GPU = %USE_GPU
PPO_runner.varying_pars = %VARYING_PARS
PPO_runner.varying_type = %VARYING_TYPE
PPO_runner.num_cores = %NUM_CORES
PPO_runner.len_series = %LEN_SERIES
PPO_runner.dt = %DT
PPO_runner.rollouts_pct_num = %ROLLOUT_PCT_NUM 
PPO_runner.epochs = %EPOCHS
PPO_runner.universal_train = %UNIVERSAL_TRAIN

# Parameters for PPO:
# ==============================================================================
PPO.seed = %SEED
PPO.gamma = 0.99 #0.8 # discounting factor for the Q target
PPO.tau = 0.98 # lambda in the original GAE paper
PPO.clip_param = 0.25 # clipping of objective function
PPO.vf_c = 0.5 # coefficient for the value error
PPO.ent_c = 0.001 # coefficient for entropy
PPO.hidden_units_value = [128,64]
PPO.hidden_units_actor = [128,64]
PPO.batch_size =  50
PPO.lr = 0.0003 # initial learning rate
PPO.activation = 'tanh'
PPO.batch_norm_input = True # batch norm at input layer level
PPO.batch_norm_value_out = False # normalize value function output
PPO.policy_type = 'continuous' #discrete or continuous
PPO.init_pol_std = 0.0 # initial policy std dev for stochasticity
PPO.min_pol_std = 0.003 # minimum policy std dev for stochasticity
PPO.std_transform = 'exp'
PPO.init_last_layers = 'normal'
PPO.optimizer_name = 'adam'
PPO.beta_1 = 0.9 # first parameter for adaptive optimizers
PPO.beta_2 = 0.999 # second parameter for adaptive optimizers
PPO.eps_opt = 1e-8 # corrective parameter for adaptive optimizers
PPO.lr_schedule = ''
PPO.exp_decay_rate = 0.999
PPO.step_size = None
PPO.store_diagnostics = False



# Parameters for ActionSpace:
# ==============================================================================
ActionSpace.action_range = %ACTION_RANGE
ActionSpace.side_only = %SIDE_ONLY
ActionSpace.zero_action = %ZERO_ACTION

# Parameters for ResActionSpace:
# ==============================================================================
ResActionSpace.action_range = %ACTION_RANGE_RES
ResActionSpace.zero_action = %ZERO_ACTION
ResActionSpace.side_only = %SIDE_ONLY

# Parameters for get_action_boundaries:
# ==============================================================================
get_action_boundaries.HalfLife = %HALFLIFE
get_action_boundaries.Startholding = %STARTHOLDING
get_action_boundaries.sigma = %SIGMA
get_action_boundaries.CostMultiplier = %COSTMULTIPLIER 
get_action_boundaries.kappa = %KAPPA
get_action_boundaries.discount_rate = %DISCOUNT_RATE 
get_action_boundaries.f_param = %F_PARAM
get_action_boundaries.qts = %QTS
get_action_boundaries.action_type = %ACTION_TYPE

# Parameters for MarketEnv:
# ==============================================================================
MarketEnv.CostMultiplier = %COSTMULTIPLIER
MarketEnv.Startholding = %STARTHOLDING
MarketEnv.discount_rate = %DISCOUNT_RATE
MarketEnv.kappa = %KAPPA
MarketEnv.sigma = %SIGMA
MarketEnv.HalfLife = %HALFLIFE
MarketEnv.kappa = %KAPPA
MarketEnv.f_param = %F_PARAM
MarketEnv.inp_type = %INP_TYPE
MarketEnv.cost_type = %COST_TYPE
MarketEnv.cm1 = %CM1
MarketEnv.cm2 = %CM2
MarketEnv.reward_type = %REWARD_TYPE
MarketEnv.cash = %CASH
MarketEnv.multiasset = %MULTIASSET
MarketEnv.corr = %CORRELATION


# Parameters for DataHandler and its related functions:
# ==============================================================================
DataHandler.datatype = %DATATYPE
DataHandler.factor_lb = %FACTOR_LB

return_sampler_GP.sigmaf = %SIGMAF
return_sampler_GP.f_param = %F_PARAM
return_sampler_GP.sigma = %SIGMA
return_sampler_GP.HalfLife = %HALFLIFE
return_sampler_GP.uncorrelated = %UNCORRELATED
return_sampler_GP.t_stud = %T_STUD
return_sampler_GP.degrees = %DEGREES
return_sampler_GP.vol = %VOL
return_sampler_GP.dt = %DT


return_sampler_garch.mean_process = %MEAN_PROCESS
return_sampler_garch.lags_mean_process = %LAGS_MEAN_PROCESS
return_sampler_garch.vol_process = %VOL_PROCESS
return_sampler_garch.distr_noise = %DISTR_NOISE
return_sampler_garch.seed = %SEED

alpha_term_structure_sampler.HalfLife = %HALFLIFE
alpha_term_structure_sampler.initial_alpha = %INITIAL_ALPHA
alpha_term_structure_sampler.f_param = %F_PARAM
alpha_term_structure_sampler.generate_plot = False
alpha_term_structure_sampler.sigmaf = %SIGMAF
alpha_term_structure_sampler.multiasset = %MULTIASSET


# Parameters for Out_sample_vs_gp:
# ==============================================================================
Out_sample_vs_gp.n_seeds = 5
Out_sample_vs_gp.N_test = %LEN_SERIES
Out_sample_vs_gp.rnd_state =  3425657

